{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6f1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fcf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #add neobert to kernel PATH\n",
    "# import sys\n",
    "# import os\n",
    "# parent_dir = os.path.abspath('..')\n",
    "# if parent_dir not in sys.path:\n",
    "\n",
    "#     sys.path.append (parent_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30b25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/gruau/OneDrive/Documents/CentraleSupelec/3A/Stage Oxford/Implementation/NeoBERT/NeoBERT_dev/src/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf612e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'neobert.model' from 'C:\\\\Users\\\\gruau\\\\OneDrive\\\\Documents\\\\CentraleSupelec\\\\3A\\\\Stage Oxford\\\\Implementation\\\\NeoBERT\\\\NeoBERT_dev\\\\src\\\\neobert\\\\model\\\\__init__.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import neobert.model as mdl\n",
    "importlib.reload(mdl)\n",
    "\n",
    "#importlib.reload(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53022c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_param_proportions(config):\n",
    "\n",
    "    model = mdl.NeoBERTLMHead(config)\n",
    "    # sd = model.state_dict()\n",
    "    # for k,v in sd.items(): \n",
    "    #     print(k,v.shape)\n",
    "        \n",
    "    # plot gradients of the gates and of the experts\n",
    "\n",
    "    num_gate_params = 0.0\n",
    "    num_expert_params = 0.0\n",
    "    num_embedding_params = 0.0\n",
    "    num_attention_params = 0.0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "            if \"gate\" in name:\n",
    "                num_gate_params += param.numel()\n",
    "            if \"experts\" in name:\n",
    "                num_expert_params += param.numel()\n",
    "            if \"model.encoder\" in name or \"decoder\" in name or \"model.positional_embedding\" in name:\n",
    "                num_embedding_params += param.numel()\n",
    "            if \"qkv\" in name or \"wo\" in name:\n",
    "                num_attention_params += param.numel()\n",
    "\n",
    "    total_num_params = num_gate_params + num_expert_params + num_embedding_params + num_attention_params\n",
    "    gate_prop= num_gate_params / total_num_params\n",
    "    attention_prop = num_attention_params / total_num_params\n",
    "    embedding_prop = num_embedding_params / total_num_params\n",
    "    expert_prop = num_expert_params / total_num_params\n",
    "    print(f\"total_num_params:{total_num_params}\")\n",
    "\n",
    "    print(f\"gate_prop:{gate_prop*100:.2f}%\")\n",
    "    print(f\"attention_prop:{attention_prop*100:.2f}%\")\n",
    "    print(f\"embedding_prop:{embedding_prop*100:.2f}%\")\n",
    "    print(f\"expert_prop:{expert_prop*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e8ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_mop_minipile_large = mdl.NeoBERTConfig(        # must be > max index in dummy_input\n",
    "    hidden_size=768,             # divisible by num_attention_heads\n",
    "    num_attention_heads=12,     # divides hidden_size evenly\n",
    "    num_hidden_layers=12,       # small number of layers\n",
    "    intermediate_size=3072,      # FFN dim\n",
    "    max_length=128,             # match seq_len\n",
    "    pad_token_id=0,            # define padding token index\n",
    "    rope=False, \n",
    "    vocab_size = 30522,\n",
    "    expert_sizes = '0,504, 616, 728, 840, 952, 1064, 1176',              # simpler\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "config_mop_minipile_inter = mdl.NeoBERTConfig(        # must be > max index in dummy_input\n",
    "    hidden_size=120,             # divisible by num_attention_heads\n",
    "    num_attention_heads=12,     # divides hidden_size evenly\n",
    "    num_hidden_layers=12,       # small number of layers\n",
    "    intermediate_size=3072,      # FFN dim\n",
    "    max_length=128,             # match seq_len\n",
    "    pad_token_id=0,            # define padding token index\n",
    "    rope=False, \n",
    "    vocab_size = 30522,\n",
    "    expert_sizes = '0, 252, 308, 364, 420, 476, 532, 588',              # simpler\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "config_hetero_minipile_large = mdl.NeoBERTConfig(        # must be > max index in dummy_input\n",
    "    hidden_size=768,             # divisible by num_attention_heads\n",
    "    num_attention_heads=12,     # divides hidden_size evenly\n",
    "    num_hidden_layers=12,       # small number of layers\n",
    "    intermediate_size=3072,      # FFN dim\n",
    "    max_length=128,             # match seq_len\n",
    "    pad_token_id=0,            # define padding token index\n",
    "    rope=False, \n",
    "    vocab_size = 30522,\n",
    "    expert_sizes = '432, 528, 624, 720, 816, 912, 1008, 1104',              # no skip connection\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "config_hetero_minipile_inter = mdl.NeoBERTConfig(        # must be > max index in dummy_input\n",
    "    hidden_size=120,             # divisible by num_attention_heads\n",
    "    num_attention_heads=12,     # divides hidden_size evenly\n",
    "    num_hidden_layers=12,       # small number of layers\n",
    "    intermediate_size=3072,      # FFN dim\n",
    "    max_length=128,             # match seq_len\n",
    "    pad_token_id=0,            # define padding token index\n",
    "    rope=False, \n",
    "    vocab_size = 30522,\n",
    "    expert_sizes = '196, 252, 308, 364, 420, 476, 532, 588',# pas complètement pertinent vu que  ca devrait plutôt dériver des params pourhetero et pas mop mais c'est pas déconnant\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "config_homo_minipile_large = mdl.NeoBERTConfig(        # must be > max index in dummy_input\n",
    "    hidden_size=768,             # divisible by num_attention_heads\n",
    "    num_attention_heads=12,     # divides hidden_size evenly\n",
    "    num_hidden_layers=12,       # small number of layers\n",
    "    intermediate_size=3072,      # FFN dim\n",
    "    max_length=128,             # match seq_len\n",
    "    pad_token_id=0,            # define padding token index\n",
    "    rope=False, \n",
    "    vocab_size = 30522,\n",
    "    expert_sizes = '768,768,768,768,768,768,768,768',              # simpler\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "config_homo_minipile_inter = mdl.NeoBERTConfig(        # must be > max index in dummy_input\n",
    "    hidden_size=120,             # divisible by num_attention_heads\n",
    "    num_attention_heads=12,     # divides hidden_size evenly\n",
    "    num_hidden_layers=12,       # small number of layers\n",
    "    intermediate_size=3072,      # FFN dim\n",
    "    max_length=128,             # match seq_len\n",
    "    pad_token_id=0,            # define padding token index\n",
    "    rope=False, \n",
    "    vocab_size = 30522,\n",
    "    expert_sizes = '300,300,300,300,300,300,300,300',              # simpler\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "configs = [    config_mop_minipile_large,    config_mop_minipile_inter,    config_hetero_minipile_large,    config_hetero_minipile_inter,    config_homo_minipile_large,    config_homo_minipile_inter]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e86282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_num_params:183334458.0\n",
      "gate_prop:0.04%\n",
      "attention_prop:15.44%\n",
      "embedding_prop:25.64%\n",
      "expert_prop:58.87%\n",
      "\n",
      "total_num_params:16472082.0\n",
      "gate_prop:0.07%\n",
      "attention_prop:4.20%\n",
      "embedding_prop:44.75%\n",
      "expert_prop:50.98%\n",
      "\n",
      "total_num_params:188642874.0\n",
      "gate_prop:0.04%\n",
      "attention_prop:15.01%\n",
      "embedding_prop:24.92%\n",
      "expert_prop:60.03%\n",
      "\n",
      "total_num_params:17025042.0\n",
      "gate_prop:0.07%\n",
      "attention_prop:4.06%\n",
      "embedding_prop:43.30%\n",
      "expert_prop:52.58%\n",
      "\n",
      "total_num_params:188642874.0\n",
      "gate_prop:0.04%\n",
      "attention_prop:15.01%\n",
      "embedding_prop:24.92%\n",
      "expert_prop:60.03%\n",
      "\n",
      "total_num_params:14986002.0\n",
      "gate_prop:0.08%\n",
      "attention_prop:4.61%\n",
      "embedding_prop:49.19%\n",
      "expert_prop:46.12%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for config in configs:\n",
    "    model_param_proportions(config)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb195cff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DictConfig' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m config_mop_minipile_large_train = \u001b[43mDictConfig\u001b[49m({\n\u001b[32m      2\u001b[39m     expert_sizes: \u001b[33m'\u001b[39m\u001b[33m0,504, 616, 728, 840, 952, 1064, 1176\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m     num_layers: \u001b[32m12\u001b[39m,\n\u001b[32m      4\u001b[39m     alpha: \u001b[32m1e-8\u001b[39m,\n\u001b[32m      5\u001b[39m     vocab_size: \u001b[32m30522\u001b[39m\n\u001b[32m      6\u001b[39m })\n\u001b[32m      8\u001b[39m config_mop_minipile_inter_train = DictConfig({\n\u001b[32m      9\u001b[39m     expert_sizes: \u001b[33m'\u001b[39m\u001b[33m0, 252, 308, 364, 420, 476, 532, 588\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     10\u001b[39m     num_layers: \u001b[32m12\u001b[39m,\n\u001b[32m     11\u001b[39m     alpha: \u001b[32m1e-8\u001b[39m,\n\u001b[32m     12\u001b[39m     vocab_size: \u001b[32m30522\u001b[39m\n\u001b[32m     13\u001b[39m })\n\u001b[32m     15\u001b[39m config_hetero_minipile_large_train = DictConfig({\n\u001b[32m     16\u001b[39m     expert_sizes: \u001b[33m'\u001b[39m\u001b[33m432, 528, 624, 720, 816, 912, 1008, 1104\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m     num_layers: \u001b[32m12\u001b[39m,\n\u001b[32m     18\u001b[39m     alpha: \u001b[32m1e-8\u001b[39m,\n\u001b[32m     19\u001b[39m     vocab_size: \u001b[32m30522\u001b[39m\n\u001b[32m     20\u001b[39m })\n",
      "\u001b[31mNameError\u001b[39m: name 'DictConfig' is not defined"
     ]
    }
   ],
   "source": [
    "config_mop_minipile_large_train = {\n",
    "    expert_sizes: '0,504, 616, 728, 840, 952, 1064, 1176',\n",
    "    num_layers: 12,\n",
    "    alpha: 1e-8,\n",
    "    vocab_size: 30522\n",
    "}\n",
    "\n",
    "config_mop_minipile_inter_train ={\n",
    "    expert_sizes: '0, 252, 308, 364, 420, 476, 532, 588',\n",
    "    num_layers: 12,\n",
    "    alpha: 1e-8,\n",
    "    vocab_size: 30522\n",
    "}\n",
    "\n",
    "config_hetero_minipile_large_train ={\n",
    "    expert_sizes: '432, 528, 624, 720, 816, 912, 1008, 1104',\n",
    "    num_layers: 12,\n",
    "    alpha: 1e-8,\n",
    "    vocab_size: 30522\n",
    "}\n",
    "\n",
    "config_hetero_minipile_inter_train = {\n",
    "    expert_sizes: '196, 252, 308, 364, 420, 476, 532, 588',\n",
    "    num_layers: 12,\n",
    "    alpha: 1e-8,\n",
    "    vocab_size: 30522\n",
    "}\n",
    "\n",
    "config_hetero_minipile_large_train = {\n",
    "    expert_sizes: '768,768,768,768,768,768,768,768',\n",
    "    num_layers: 12,\n",
    "    alpha: 1e-8,\n",
    "    vocab_size: 30522\n",
    "}\n",
    "\n",
    "config_hetero_minipile_inter_train = {\n",
    "    expert_sizes: '300,300,300,300,300,300,300,300',\n",
    "    num_layers: 12,\n",
    "    alpha: 1e-8,\n",
    "    vocab_size: 30522\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c9ab86",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 8 (2564262645.py, line 10)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef initial_penalty_loss(**cfg_hetero):\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 8\n"
     ]
    }
   ],
   "source": [
    "def initial_expert_loss(**cfg_mop):\n",
    "    somme = sum(expert_sizes**2)\n",
    "    num_experts = len(expert_sizes)\n",
    "    mlm_loss = -np.log(1/vocab_size)\n",
    "    initial_expert_loss = (num_layers*somme*alpha)/(num_experts*mlm_loss)\n",
    "    return  initial_expert_loss\n",
    "\n",
    "def initial_load_balancing_loss(**cfg_homo):\n",
    "\n",
    "def initial_penalty_loss(**cfg_hetero):\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "812ddc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_expert_loss: 8.987599716798003e-05\n",
      "initial_penalty_loss: 0.0018007499999999998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#expert_loss\n",
    "expert_sizes = np.array([0, 252, 308, 364, 420, 476, 532, 588])\n",
    "expo_coeff=1.5\n",
    "somme = sum(expert_sizes**expo_coeff)\n",
    "num_layers = 12\n",
    "num_experts = len(expert_sizes)\n",
    "alpha = 1e-8\n",
    "vocab_size = 30522\n",
    "\n",
    "\n",
    "#expert_loss\n",
    "mlm_loss = -np.log(1/vocab_size)\n",
    "initial_expert_loss = (num_layers*somme*alpha)/(num_experts*mlm_loss)\n",
    "print(\"initial_expert_loss:\", initial_expert_loss)\n",
    "\n",
    "#penalty_loss\n",
    "expert_sizes = np.array([196, 252, 308, 364, 420, 476, 532, 588])\n",
    "coeff_penalty =1.5e-6 #1e-6\n",
    "top_p = 0.4\n",
    "# 0.125*4 = 0.5\n",
    "# 0.125*3 = 0.375\n",
    "# donc a priori il faut 4 experts activés au début. \n",
    "num_experts = len(expert_sizes)\n",
    "mean_expert_size = sum(expert_sizes)/num_experts\n",
    "\n",
    "prop_3 = 3.75/4\n",
    "prop_4 = 1-3.75/4\n",
    "\n",
    "initial_penalty_loss = coeff_penalty *(prop_3*3+prop_4*4)*mean_expert_size\n",
    "print(\"initial_penalty_loss:\", initial_penalty_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65945cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007684962216220229\n"
     ]
    }
   ],
   "source": [
    "\n",
    "expert_sizes = np.array([0,504, 616, 728, 840, 952, 1064, 1176])\n",
    "somme = sum(expert_sizes**expo_coeff)\n",
    "num_layers = 12\n",
    "num_experts = len(expert_sizes)\n",
    "alpha = 1e-8\n",
    "vocab_size = 30522\n",
    "mlm_loss = -np.log(1/vocab_size)\n",
    "print((num_layers*somme*alpha)/(num_experts*mlm_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
