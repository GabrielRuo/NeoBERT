{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c6f1b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fcf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #add neobert to kernel PATH\n",
    "# import sys\n",
    "# import os\n",
    "# parent_dir = os.path.abspath('..')\n",
    "# if parent_dir not in sys.path:\n",
    "\n",
    "#     sys.path.append (parent_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30b25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/gruau/OneDrive/Documents/CentraleSupelec/3A/Stage Oxford/Implementation/NeoBERT/NeoBERT_dev/src/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddf612e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'neobert.model' from 'C:\\\\Users\\\\gruau\\\\OneDrive\\\\Documents\\\\CentraleSupelec\\\\3A\\\\Stage Oxford\\\\Implementation\\\\NeoBERT\\\\NeoBERT_dev\\\\src\\\\neobert\\\\model\\\\__init__.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import neobert.model as mdl\n",
    "importlib.reload(mdl)\n",
    "\n",
    "#importlib.reload(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a52370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] WON'T CONVERT forward C:\\Users/gruau/OneDrive/Documents/CentraleSupelec/3A/Stage Oxford/Implementation/NeoBERT/NeoBERT_dev/src\\neobert\\model\\rmsnorm.py line 23 \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] due to: \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 130, in check_compiler_exist_windows\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     subprocess.check_output([compiler, \"/help\"], stderr=subprocess.STDOUT)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 466, in check_output\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     with Popen(*popenargs, **kwargs) as process:\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1446, in _call_user_compiler\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 129, in __call__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\__init__.py\", line 2234, in __call__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1521, in compile_fx\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return aot_autograd(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 72, in __call__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                ^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1350, in fw_compiler_base\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1421, in _fw_compiler_base\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return inner_compile(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 475, in compile_fx_inner\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 85, in debug_wrapper\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 661, in _compile_fx_inner\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1334, in load\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 570, in codegen_and_compile\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1913, in compile_to_fn\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self.compile_to_module().call\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1839, in compile_to_module\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._compile_to_module()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1845, in _compile_to_module\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                                              ^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1784, in codegen\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.scheduler.codegen()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3383, in codegen\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._codegen()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3461, in _codegen\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.get_backend(device).codegen_node(node)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4464, in codegen_node\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.codegen_outer_loop_node(node)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4438, in codegen_outer_loop_node\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     if not try_outer_loop_fusion_with_local_buf(node):\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4413, in try_outer_loop_fusion_with_local_buf\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     cpp_kernel_proxy = CppKernelProxy(kernel_group)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3466, in __init__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 360, in pick_vec_isa\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     _valid_vec_isa_list: List[VecISA] = valid_vec_isa_list()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 350, in valid_vec_isa_list\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa:\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                                                             ^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 148, in __bool__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self.check_build(VecISA._avx_code)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 101, in check_build\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     extra=_get_isa_dry_compile_fingerprint(self._arch_flags),\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 27, in _get_isa_dry_compile_fingerprint\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiler_info = get_compiler_version_info(get_cpp_compiler())\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                               ^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 144, in get_cpp_compiler\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     check_compiler_exist_windows(compiler)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 135, in check_compiler_exist_windows\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     raise RuntimeError(f\"Compiler: {compiler} is not found.\") from exc\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] RuntimeError: Compiler: cl is not found.\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1064, in __call__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     result = self._inner_convert(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 526, in __call__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _compile(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 924, in _compile\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 666, in compile_inner\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_utils_internal.py\", line 87, in wrapper_function\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return function(*args, **kwargs)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 699, in _compile_inner\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     transformations(instructions, code_options)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 219, in _fn\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return fn(*args, **kwargs)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 634, in transform\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     tracer.run()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2796, in run\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     super().run()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 983, in run\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     while self.step():\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]           ^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 895, in step\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self._return(inst)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2972, in _return\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.output.compile_subgraph(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1117, in compile_subgraph\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1416, in call_user_compiler\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1465, in _call_user_compiler\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] RuntimeError: Compiler: cl is not found.\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 130, in check_compiler_exist_windows\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     subprocess.check_output([compiler, \"/help\"], stderr=subprocess.STDOUT)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 466, in check_output\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     with Popen(*popenargs, **kwargs) as process:\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1446, in _call_user_compiler\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 129, in __call__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\__init__.py\", line 2234, in __call__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1521, in compile_fx\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return aot_autograd(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 72, in __call__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                ^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 179, in aot_dispatch_base\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fw = compiler(fw_module, updated_flat_args)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1350, in fw_compiler_base\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1421, in _fw_compiler_base\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return inner_compile(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 475, in compile_fx_inner\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 85, in debug_wrapper\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 661, in _compile_fx_inner\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1334, in load\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 570, in codegen_and_compile\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1913, in compile_to_fn\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self.compile_to_module().call\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1839, in compile_to_module\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._compile_to_module()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1845, in _compile_to_module\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                                              ^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1784, in codegen\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.scheduler.codegen()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3383, in codegen\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._codegen()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3461, in _codegen\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.get_backend(device).codegen_node(node)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4464, in codegen_node\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.codegen_outer_loop_node(node)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4438, in codegen_outer_loop_node\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     if not try_outer_loop_fusion_with_local_buf(node):\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4413, in try_outer_loop_fusion_with_local_buf\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     cpp_kernel_proxy = CppKernelProxy(kernel_group)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3466, in __init__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 360, in pick_vec_isa\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     _valid_vec_isa_list: List[VecISA] = valid_vec_isa_list()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 350, in valid_vec_isa_list\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa:\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                                                             ^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 148, in __bool__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self.check_build(VecISA._avx_code)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 101, in check_build\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     extra=_get_isa_dry_compile_fingerprint(self._arch_flags),\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 27, in _get_isa_dry_compile_fingerprint\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiler_info = get_compiler_version_info(get_cpp_compiler())\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                               ^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 144, in get_cpp_compiler\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     check_compiler_exist_windows(compiler)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 135, in check_compiler_exist_windows\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     raise RuntimeError(f\"Compiler: {compiler} is not found.\") from exc\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] RuntimeError: Compiler: cl is not found.\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1064, in __call__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     result = self._inner_convert(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 526, in __call__\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _compile(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 924, in _compile\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 666, in compile_inner\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_utils_internal.py\", line 87, in wrapper_function\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return function(*args, **kwargs)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 699, in _compile_inner\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     transformations(instructions, code_options)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 219, in _fn\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return fn(*args, **kwargs)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 634, in transform\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     tracer.run()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2796, in run\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     super().run()\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 983, in run\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     while self.step():\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]           ^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 895, in step\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2987, in RETURN_VALUE\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self._return(inst)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2972, in _return\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.output.compile_subgraph(\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1117, in compile_subgraph\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, list(reversed(stack_values)), root)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1416, in call_user_compiler\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1465, in _call_user_compiler\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] RuntimeError: Compiler: cl is not found.\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0730 10:54:34.700000 46140 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 16, 100])\n",
      "Expert usage loss: tensor(7864528.5000)\n"
     ]
    }
   ],
   "source": [
    "import neobert.model as mdl  # or your actual import if different\n",
    "\n",
    "# Create a minimal config\n",
    "config = mdl.NeoBERTConfig(\n",
    "    vocab_size=100,            # must be > max index in dummy_input\n",
    "    hidden_size=8,             # divisible by num_attention_heads\n",
    "    num_attention_heads=2,     # divides hidden_size evenly\n",
    "    num_hidden_layers=2,       # small number of layers\n",
    "    intermediate_size=16,      # FFN dim\n",
    "    max_length=16,             # match seq_len\n",
    "    pad_token_id=0,            # define padding token index\n",
    "    rope=False, \n",
    "    expert_sizes = '0,1152, 1408, 1664, 1920, 2176, 2432, 2688, 2944',              # simpler\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "# Instantiate model\n",
    "model = mdl.NeoBERTLMHead(config)\n",
    "\n",
    "# Generate dummy input (ensure all token indices < vocab_size)\n",
    "batch_size = 2\n",
    "seq_len = 16\n",
    "\n",
    "# Token indices from 1 to vocab_size-1 (reserve 0 for pad_token)\n",
    "dummy_input = torch.randint(1, config.vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Add some padding tokens at the end (optional)\n",
    "dummy_input[:, -2:] = config.pad_token_id  # last 2 tokens are padding\n",
    "\n",
    "# Generate additive pad mask: 0.0 for valid tokens, -inf for padding\n",
    "pad_mask = (dummy_input != config.pad_token_id).float()\n",
    "pad_mask = torch.where(pad_mask == 1, 0.0, float('-inf'))\n",
    "\n",
    "# Run forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input, pad_mask=pad_mask)\n",
    "\n",
    "# Check outputs\n",
    "print(\"Logits shape:\", output[\"logits\"].shape)\n",
    "print(\"Expert usage loss:\", output[\"expert_usage_loss\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239cb36",
   "metadata": {},
   "source": [
    "Look into the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0af927de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.encoder.weight torch.Size([32768, 768])\n",
      "model.transformer_encoder.0.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.0.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.0.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.0.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.0.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.0.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.0.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.0.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.0.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.1.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.1.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.1.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.1.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.1.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.1.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.1.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.1.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.1.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.2.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.2.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.2.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.2.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.2.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.2.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.2.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.2.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.2.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.3.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.3.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.3.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.3.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.3.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.3.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.3.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.3.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.3.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.4.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.4.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.4.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.4.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.4.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.4.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.4.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.4.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.4.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.5.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.5.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.5.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.5.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.5.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.5.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.5.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.5.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.5.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.6.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.6.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.6.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.6.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.6.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.6.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.6.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.6.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.6.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.7.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.7.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.7.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.7.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.7.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.7.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.7.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.7.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.7.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.8.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.8.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.8.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.8.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.8.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.8.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.8.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.8.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.8.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.9.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.9.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.9.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.9.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.9.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.9.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.9.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.9.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.9.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.10.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.10.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.10.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.10.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.10.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.10.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.10.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.10.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.10.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.11.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.11.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.11.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.11.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.11.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.11.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.11.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.11.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.11.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.12.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.12.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.12.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.12.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.12.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.12.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.12.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.12.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.12.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.13.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.13.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.13.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.13.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.13.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.13.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.13.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.13.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.13.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.14.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.14.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.14.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.14.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.14.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.14.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.14.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.14.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.14.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.15.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.15.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.15.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.15.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.15.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.15.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.15.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.15.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.15.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.16.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.16.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.16.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.16.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.16.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.16.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.16.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.16.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.16.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.17.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.17.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.17.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.17.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.17.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.17.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.17.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.17.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.17.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.18.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.18.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.18.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.18.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.18.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.18.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.18.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.18.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.18.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.19.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.19.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.19.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.19.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.19.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.19.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.19.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.19.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.19.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.20.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.20.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.20.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.20.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.20.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.20.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.20.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.20.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.20.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.21.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.21.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.21.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.21.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.21.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.21.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.21.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.21.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.21.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.22.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.22.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.22.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.22.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.22.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.22.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.22.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.22.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.22.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.23.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.23.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.23.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.23.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.23.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.23.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.23.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.23.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.23.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.24.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.24.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.24.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.24.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.24.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.24.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.24.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.24.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.24.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.25.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.25.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.25.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.25.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.25.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.25.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.25.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.25.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.25.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.26.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.26.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.26.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.26.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.26.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.26.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.26.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.26.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.26.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.transformer_encoder.27.qkv.weight torch.Size([2304, 768])\n",
      "model.transformer_encoder.27.wo.weight torch.Size([768, 768])\n",
      "model.transformer_encoder.27.attention_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.27.expert_norm.weight torch.Size([768])\n",
      "model.transformer_encoder.27.block_moe.gate.weight torch.Size([3, 768])\n",
      "model.transformer_encoder.27.block_moe.experts.1.ffn.in_proj.weight torch.Size([2048, 768])\n",
      "model.transformer_encoder.27.block_moe.experts.1.ffn.out_proj.weight torch.Size([768, 1024])\n",
      "model.transformer_encoder.27.block_moe.experts.2.ffn.in_proj.weight torch.Size([4096, 768])\n",
      "model.transformer_encoder.27.block_moe.experts.2.ffn.out_proj.weight torch.Size([768, 2048])\n",
      "model.layer_norm.weight torch.Size([768])\n",
      "decoder.weight torch.Size([32768, 768])\n",
      "decoder.bias torch.Size([32768])\n"
     ]
    }
   ],
   "source": [
    "import neobert.model as mdl  # or your actual import if different\n",
    "\n",
    "# Create a minimal config\n",
    "config = mdl.NeoBERTConfig(\n",
    ")\n",
    "# Instantiate model\n",
    "model = mdl.NeoBERTLMHead(config)\n",
    "\n",
    "\n",
    "sd = model.state_dict()\n",
    "for k,v in sd.items(): \n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expert_mask(gate_logits,cfg):\n",
    "    if isinstance(gate_logits, tuple):\n",
    "        compute_device = gate_logits[0].device\n",
    "        stacked_gate_logits = torch.stack([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)#n_layers, batch_size*seq_length, n_experts\n",
    "        _,_, num_experts = stacked_gate_logits.shape\n",
    "    routing_weights = torch.nn.functional.softmax(stacked_gate_logits, dim=-1)\n",
    "\n",
    "\n",
    "    if cfg.model.routing_strategy == \"top_k\":\n",
    "        top_k = cfg.model.num_experts_per_tok_inference #if we wanted  to use top_k for heterogeneous moe\n",
    "        _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "        target_expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts).sum(2)#n_layers, batch_size*seq_length, n_experts\n",
    "    \n",
    "    elif cfg.model.routing_strategy == \"top_p\":\n",
    "        top_p = cfg.model.min_expert_cumprob_per_token\n",
    "        sorted_weights, sorted_indices = torch.sort(routing_weights, dim=-1, descending=False)\n",
    "\n",
    "        cum_probs = sorted_weights.cumsum(dim=-1)\n",
    "        mask = cum_probs > 1 - top_p\n",
    "\n",
    "        unsorted_mask = torch.zeros_like(mask, dtype=torch.bool)\n",
    "        target_expert_mask = unsorted_mask.scatter(dim=-1, index=sorted_indices, src=mask) #n_layers, batch_size*seq_length, n_experts\n",
    "\n",
    "    return target_expert_mask # n_layers, batch_size*n_seq, n_experts\n",
    "\n",
    "\n",
    "neobert_model_output = neobert_model(batch[\"input_ids\"], batch.get(\"attention_mask\", None), output_expert_usage_loss=False, output_router_logits=True)\n",
    "target_gate_logits = neobert_model_output['router_logits'] #tuple  of n_layers of batch_size*seq_length,  n_experts\n",
    "\n",
    "target_expert_mask = get_expert_mask(target_gate_logits, cfg) # n_layers, batch_size*n_seq, n_experts\n",
    "\n",
    "predicted_routed_logits = compiled_BERTpredictor(input_ids=batch[\"input_ids\"])\n",
    "predicted_routed_logits = torch.stack(predicted_routed_logits, dim=0) #n_layers, batch_size*seq_length, n_experts\n",
    "\n",
    "#ignore padded tokens\n",
    "pad_mask = batch.get(\"attention_mask\", None)\n",
    "if pad_mask is not None:\n",
    "    pad_mask = pad_mask.view(-1, 1)\n",
    "    pad_mask = (pad_mask != float(\"-inf\")).squeeze(-1)\n",
    "    predicted_routed_logits = predicted_routed_logits[:,pad_mask,:]\n",
    "    target_expert_mask = target_expert_mask[:,pad_mask,:]\n",
    "\n",
    "#reshape \n",
    "predicted_routed_logits = predicted_routed_logits.view(-1, predicted_routed_logits.size(-1)) #n_layers*batch_size*seq_length, n_experts\n",
    "target_expert_mask = target_expert_mask.view(-1, target_expert_mask.size(-1)) #n_layers*batch_size*seq_length, n_experts\n",
    "\n",
    "# Under the no_sync context manager, PyTorch will skip synchronizing the gradients when .backward() is\n",
    "# called, and the first call to .backward() outside this context manager will trigger the synchronization.\n",
    "# Accumulating manually gives more flexibility and is compatible with TPUs.\n",
    "# if metrics[\"train/batches\"] % cfg.trainer.gradient_accumulation_steps != 0:\n",
    "#     with accelerator.no_sync(BERTpredictor):\n",
    "#         loss = loss_fn(predicted_routed_logits, target_expert_mask*1.0)\n",
    "#         loss.backward()\n",
    "\n",
    "loss = loss_fn(predicted_routed_logits, target_expert_mask*1.0)\n",
    "accelerator.backward(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b16f50cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([4, 3])\n",
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652],\n",
      "        [0.0900, 0.2447, 0.6652]])\n",
      "torch.Size([4])\n",
      "tensor([0.8324, 0.8324, 0.8324, 0.8324])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "b = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "c = ()\n",
    "d = c + (a,)+ (b,)\n",
    "# print(type(d))\n",
    "# print(d)\n",
    "\n",
    "e = torch.cat([tens for tens in d], dim=0)\n",
    "\n",
    "print(e.shape)\n",
    "print(e)\n",
    "\n",
    "f = torch.nn.functional.softmax(e.float(), dim=-1)\n",
    "print(f.shape)\n",
    "print(f)\n",
    "\n",
    "\n",
    "entropy = torch.sum(-f * torch.log(f + 1e-10), dim=-1)\n",
    "print(entropy.shape)\n",
    "print(entropy)\n",
    "# g = torch.mean(f, dim=0)\n",
    "# # print(g.shape)\n",
    "# # print(g)\n",
    "# _, selected_experts = torch.topk(f, 2, dim=-1)\n",
    "\n",
    "# expert_mask = torch.nn.functional.one_hot(selected_experts, 3)\n",
    "# expert_size = [int(expert_size) for expert_size in '2,4,6'.split(',')]\n",
    "# print(expert_size)\n",
    "# expert_mask = expert_mask *torch.tensor(expert_size)\n",
    "# print(expert_mask.shape)\n",
    "# print(expert_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "749cad3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('a,b,c'.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f56c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 0, 0], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([10, 9, 7, 4, 2], dtype=torch.float)\n",
    "p = 20.0\n",
    "\n",
    "# Step 1: Cumulative sum\n",
    "cumsum = torch.cumsum(x, dim=0)\n",
    "\n",
    "# Step 2: Find first index where cumsum >= p\n",
    " \n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60078754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gruau\\AppData\\Local\\Temp\\ipykernel_35176\\2055800616.py:2: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
      "  sorted_indices[mask]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices = torch.tensor([0, 1, 2, 3, 4])\n",
    "sorted_indices[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03e017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544842900\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(num_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546b077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3907, 0.2894, 0.3199],\n",
      "        [0.3006, 0.3322, 0.3672]])\n",
      "tensor([[0.2894, 0.3199, 0.3907],\n",
      "        [0.3006, 0.3322, 0.3672]]) tensor([[1, 2, 0],\n",
      "        [0, 1, 2]])\n",
      "mask tensor([[False,  True,  True],\n",
      "        [False,  True,  True]])\n",
      "tensor([[0.0000, 0.2894, 0.3199],\n",
      "        [0.0000, 0.3322, 0.3672]])\n",
      "expert hittd tensor([2, 0, 1, 2])\n",
      "tensor([0.3199, 0.3907, 0.3322, 0.3672])\n"
     ]
    }
   ],
   "source": [
    "k \n",
    "\n",
    "print(\"mask\", mask)\n",
    "masked_weights = mask*routing_weights\n",
    "\n",
    "\n",
    "print(\"expert hittd\",expert_hitted)\n",
    "print(expert_hitted_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5e098b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing weights:\n",
      " tensor([[0.3907, 0.2894, 0.3199],\n",
      "        [0.3006, 0.3322, 0.3672],\n",
      "        [0.4018, 0.2693, 0.3289],\n",
      "        [0.2603, 0.3514, 0.3883]])\n",
      "tensor([[[1, 0, 1, 0],\n",
      "         [0, 0, 0, 0]],\n",
      "\n",
      "        [[0, 0, 0, 0],\n",
      "         [0, 1, 0, 1]],\n",
      "\n",
      "        [[0, 1, 0, 1],\n",
      "         [1, 0, 1, 0]]])\n",
      "Expert hitted: tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n"
     ]
    }
   ],
   "source": [
    "top_p = 0.4\n",
    "\n",
    "# Step 1: Softmax routing weights\n",
    "routing_weights = torch.nn.functional.softmax(torch.tensor([[0.5, 0.2, 0.3], [0.4, 0.5, 0.6],[0.5, 0.1, 0.3], [0.2, 0.5, 0.6]], dtype=torch.float), dim=-1)\n",
    "print(\"Routing weights:\\n\", routing_weights)\n",
    "\n",
    "\n",
    "routing_weights, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "\n",
    "num_experts = 3\n",
    "# One hot encode the selected experts to create an expert mask\n",
    "# this will be used to easily index which expert is going to be sollicitated\n",
    "expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=num_experts).permute(2, 1, 0)\n",
    "print(expert_mask)\n",
    "expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
    "print(\"Expert hitted:\", expert_hitted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "465fdbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing weights:\n",
      " tensor([[0.3907, 0.2894, 0.3199],\n",
      "        [0.3006, 0.3322, 0.3672],\n",
      "        [0.4018, 0.2693, 0.3289],\n",
      "        [0.2603, 0.3514, 0.3883]])\n",
      "hidden states:\n",
      " tensor([[-0.1232, -0.7873],\n",
      "        [-0.1052, -0.0381],\n",
      "        [-0.0502,  2.0465],\n",
      "        [-0.9725,  0.6955]])\n",
      "Sorted weights:\n",
      " tensor([[0.2894, 0.3199, 0.3907],\n",
      "        [0.3006, 0.3322, 0.3672],\n",
      "        [0.2693, 0.3289, 0.4018],\n",
      "        [0.2603, 0.3514, 0.3883]])\n",
      "Sorted indices:\n",
      " tensor([[1, 2, 0],\n",
      "        [0, 1, 2],\n",
      "        [1, 2, 0],\n",
      "        [0, 1, 2]])\n",
      "expert_mask tensor([[ True, False,  True, False],\n",
      "        [False,  True, False,  True],\n",
      "        [ True,  True, False,  True]])\n",
      "(tensor([0, 2]),)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     25\u001b[39m token_idx= torch.where(expert_mask[expert_idx].squeeze(\u001b[32m0\u001b[39m))\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(idx)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m current_state = \u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtoken_idx\u001b[49m\u001b[43m]\u001b[49m.reshape(-\u001b[32m1\u001b[39m, hidden_dim)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(current_state)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m#current_hidden_states = expert_layer(current_state) * routing_weights[token_idx, expert_idx, None]\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "top_p = 0.4\n",
    "batch_size = 1; sequence_length = 4; hidden_dim = 2\n",
    "# Step 1: Softmax routing weights\n",
    "routing_weights = torch.nn.functional.softmax(torch.tensor([[0.5, 0.2, 0.3], [0.4, 0.5, 0.6],[0.5, 0.1, 0.3], [0.2, 0.5, 0.6]], dtype=torch.float), dim=-1)\n",
    "hidden_states = torch.randn(batch_size*sequence_length,hidden_dim)  # Simulated current state for demonstration\n",
    "print(\"Routing weights:\\n\", routing_weights)\n",
    "print(\"hidden states:\\n\", hidden_states)\n",
    "\n",
    "# Step 2: Sort weights and indices\n",
    "sorted_weights, sorted_indices = torch.sort(routing_weights, dim=-1, descending=False)\n",
    "print(\"Sorted weights:\\n\", sorted_weights)\n",
    "print(\"Sorted indices:\\n\", sorted_indices)\n",
    "\n",
    "# Step 3: Compute cumulative probabilities\n",
    "cum_probs = sorted_weights.cumsum(dim=-1)\n",
    "# Step 4: Mask for top-p\n",
    "mask = cum_probs > 1 - top_p  # same shape as routing_weights\n",
    "unsorted_mask = torch.zeros_like(mask, dtype=mask.dtype)\n",
    "expert_mask = unsorted_mask.scatter_(dim=-1, index=sorted_indices, src=mask).permute(1, 0)\n",
    "print(\"expert_mask\", expert_mask)\n",
    "expert_hitted = torch.greater(expert_mask.sum(dim=(-1)), 0).nonzero()\n",
    "for expert_idx in expert_hitted:\n",
    "    token_idx= torch.where(expert_mask[expert_idx].squeeze(0))\n",
    "    print(idx)\n",
    "    current_state = hidden_states[None,token_idx].reshape(-1, hidden_dim)\n",
    "    print(current_state)\n",
    "    #current_hidden_states = expert_layer(current_state) * routing_weights[token_idx, expert_idx, None]\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "99859af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.randn(batch_size * sequence_length, hidden_dim)\n",
    "# Dummy expert modules (simulate expert layers)\n",
    "class DummyExpert(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "experts = torch.nn.ModuleList([DummyExpert() for _ in range(num_experts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d458a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "Final routed hidden states:\n",
      " tensor([[ 0.1863, -0.1310],\n",
      "        [ 0.1626, -0.4599],\n",
      "        [ 0.2274, -0.0569],\n",
      "        [ 0.5481,  0.0104]], grad_fn=<IndexAddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "top_p = 0.4\n",
    "num_experts = 3\n",
    "batch_size = 1\n",
    "sequence_length = 4\n",
    "hidden_dim = 2\n",
    "\n",
    "\n",
    "\n",
    "# Routing weights and hidden states\n",
    "routing_logits = torch.tensor([\n",
    "    [0.5, 0.2, 0.3],\n",
    "    [0.4, 0.5, 0.6],\n",
    "    [0.5, 0.1, 0.3],\n",
    "    [0.2, 0.5, 0.6]\n",
    "], dtype=torch.float)\n",
    "\n",
    "routing_weights = F.softmax(routing_logits, dim=-1)\n",
    "\n",
    "\n",
    "# Sort and compute cumulative probability\n",
    "\n",
    "\n",
    "print(\"Final routed hidden states:\\n\", final_hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f4eb4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final tensor of expert indices per token (padded):\n",
      "tensor([[ 4,  1,  0, -1],\n",
      "        [ 1,  3,  2,  4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "top_p = 0.7\n",
    "\n",
    "routing_weights = torch.nn.functional.softmax(\n",
    "    torch.tensor([[0.9, 0.8, 0.1,0.1,0.2], [0.1, 0.1, 0.2,0.1,0.2]], dtype=torch.float), dim=-1)\n",
    "\n",
    "# Sort the weights to prepare for cumulative probability\n",
    "sorted_weights, sorted_indices = torch.sort(routing_weights, dim=-1, descending=False)\n",
    "cum_probs = sorted_weights.cumsum(dim=-1)\n",
    "\n",
    "# Build mask for top-p selection\n",
    "mask = cum_probs > 1 - top_p\n",
    "\n",
    "# Apply mask to sorted_indices\n",
    "# Get expert indices per token based on top-p mask\n",
    "expert_indices_per_token = []\n",
    "max_selected = 0  # to determine padding width\n",
    "\n",
    "for token_mask, token_sorted_indices in zip(mask, sorted_indices):\n",
    "    selected = token_sorted_indices[token_mask]\n",
    "    expert_indices_per_token.append(selected)\n",
    "    max_selected = max(max_selected, selected.numel())\n",
    "\n",
    "# Pad to build final tensor\n",
    "padded_indices = torch.full((len(expert_indices_per_token), max_selected), -1, dtype=torch.long)\n",
    "\n",
    "for i, selected in enumerate(expert_indices_per_token):\n",
    "    padded_indices[i, :selected.numel()] = selected\n",
    "\n",
    "print(\"Final tensor of expert indices per token (padded):\")\n",
    "print(padded_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674dcf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "top_p = 0.4\n",
    "\n",
    "routing_weights = torch.nn.functional.softmax(\n",
    "    torch.tensor([[0.5, 0.2, 0.3], [0.4, 0.5, 0.6]], dtype=torch.float), dim=-1)\n",
    "\n",
    "# Sort the weights to prepare for cumulative probability\n",
    "sorted_weights, sorted_indices = torch.sort(routing_weights, dim=-1, descending=False)\n",
    "cum_probs = sorted_weights.cumsum(dim=-1)\n",
    "\n",
    "# Build mask for top-p selection\n",
    "mask = cum_probs > 1 - top_p\n",
    "\n",
    "# Apply mask to sorted_indices\n",
    "# Get expert indices per token based on top-p mask\n",
    "expert_indices_per_token = []\n",
    "max_selected = 0  # to determine padding width\n",
    "\n",
    "for token_mask, token_sorted_indices in zip(mask, sorted_indices):\n",
    "    selected = token_sorted_indices[token_mask]\n",
    "    expert_indices_per_token.append(selected)\n",
    "    max_selected = max(max_selected, selected.numel())\n",
    "\n",
    "# Pad to build final tensor\n",
    "padded_indices = torch.full((len(expert_indices_per_token), max_selected), -1, dtype=torch.long)\n",
    "\n",
    "for i, selected in enumerate(expert_indices_per_token):\n",
    "    padded_indices[i, :selected.numel()] = selected\n",
    "\n",
    "print(\"Final tensor of expert indices per token (padded):\")\n",
    "print(padded_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf08cd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3907, 0.3199],\n",
      "        [0.3672, 0.3322]]) tensor([[0, 1],\n",
      "        [0, 1]])\n"
     ]
    }
   ],
   "source": [
    "top_k = 2\n",
    "routing_weights, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "print(routing_weights, selected_experts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9b6906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gruau\\anaconda3\\python312.zip\n",
      "C:\\Users\\gruau\\anaconda3\\DLLs\n",
      "C:\\Users\\gruau\\anaconda3\\Lib\n",
      "C:\\Users\\gruau\\anaconda3\n",
      "c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\n",
      "\n",
      "c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\n",
      "c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\win32\n",
      "c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\win32\\lib\n",
      "c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\neoBERT_env\\Lib\\site-packages\\Pythonwin\n",
      "C:/Users/gruau/OneDrive/Documents/CentraleSupelec/3A/Stage Oxford/Implementation/NeoBERT/NeoBERT_dev/src/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"\\n\".join(sys.path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07e5ebe",
   "metadata": {},
   "source": [
    "Test top k routing/ NEED TO add MoeBLock to the init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efd317f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'neobert.model' from 'C:\\\\Users\\\\gruau\\\\OneDrive\\\\Documents\\\\CentraleSupelec\\\\3A\\\\Stage Oxford\\\\Implementation\\\\NeoBERT\\\\NeoBERT_dev\\\\src\\\\neobert\\\\model\\\\__init__.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import neobert.model as mdl\n",
    "importlib.reload(mdl)\n",
    "\n",
    "#importlib.reload(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9605056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_input:  tensor([[[-0.1565,  0.0036],\n",
      "         [-0.8491,  1.5031],\n",
      "         [-1.1938, -0.4445],\n",
      "         [ 0.0880, -0.1160]]])\n",
      "rtouter_logits tensor([[-0.0743,  0.6403, -0.4663],\n",
      "        [ 0.0646, -0.5416, -0.9011],\n",
      "        [ 0.1294,  0.7737, -0.2321],\n",
      "        [-0.1226,  0.7680, -0.4474]], grad_fn=<AddmmBackward0>)\n",
      "hidden_states tensor([[-0.1565,  0.0036],\n",
      "        [-0.8491,  1.5031],\n",
      "        [-1.1938, -0.4445],\n",
      "        [ 0.0880, -0.1160]])\n",
      "routing_weights tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<DivBackward0>)\n",
      "expert_idx tensor([0])\n",
      "expert_layer Expert()\n",
      "current_hidden_states tensor([[-0.8491,  1.5031]], grad_fn=<MulBackward0>)\n",
      "expert_idx tensor([1])\n",
      "expert_layer Expert(\n",
      "  (ffn): SwiGLU(\n",
      "    (in_proj): Linear(in_features=2, out_features=2048, bias=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=2, bias=False)\n",
      "  )\n",
      ")\n",
      "current_hidden_states tensor([[1.0416e-04, 2.0962e-04],\n",
      "        [2.2493e-03, 4.1346e-02],\n",
      "        [4.9990e-04, 8.1566e-06]], grad_fn=<MulBackward0>)\n",
      "final_hidden_states tensor([[[ 1.0416e-04,  2.0962e-04],\n",
      "         [-8.4912e-01,  1.5031e+00],\n",
      "         [ 2.2493e-03,  4.1346e-02],\n",
      "         [ 4.9990e-04,  8.1566e-06]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.0416e-04,  2.0962e-04],\n",
       "          [-8.4912e-01,  1.5031e+00],\n",
       "          [ 2.2493e-03,  4.1346e-02],\n",
       "          [ 4.9990e-04,  8.1566e-06]]], grad_fn=<ViewBackward0>),\n",
       " tensor([[-0.0743,  0.6403, -0.4663],\n",
       "         [ 0.0646, -0.5416, -0.9011],\n",
       "         [ 0.1294,  0.7737, -0.2321],\n",
       "         [-0.1226,  0.7680, -0.4474]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a minimal config\n",
    "config = mdl.NeoBERTConfig(\n",
    "    vocab_size=100,            # must be > max index in dummy_input\n",
    "    hidden_size=2,             # divisible by num_attention_heads\n",
    "    num_attention_heads=2,     # divides hidden_size evenly\n",
    "    num_hidden_layers=2,       # small number of layers\n",
    "    intermediate_size=16,      # FFN dim\n",
    "    max_length=16,             # match seq_len\n",
    "    pad_token_id=0,\n",
    "    routing_strategy=\"top_k\",\n",
    "    num_experts_per_tok_inference=1,                                       # define padding token index\n",
    "    rope=False,                # simpler\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "# Instantiate mode\n",
    "\n",
    "# Generate dummy input (ensure all token indices < vocab_si14ze)\n",
    "batch_size = 1\n",
    "seq_len = 4\n",
    "\n",
    "test_input = torch.randn(batch_size,seq_len,config.hidden_size)\n",
    "test_input_shape = test_input.shape\n",
    "print(\"test_input: \", test_input)\n",
    "\n",
    "MoeBlock = mdl.MoEBlock(config)\n",
    "MoeBlock.eval()\n",
    "\n",
    "MoeBlock(hidden_states = test_input, output_expert_usage_loss = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de3c5d6",
   "metadata": {},
   "source": [
    "test topp routing:NEED to add MoEBlock to the __init__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ec4e231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_input:  tensor([[[ 0.3110,  0.9468],\n",
      "         [ 0.3964, -0.0248],\n",
      "         [-0.8394,  0.3342],\n",
      "         [-0.5307,  0.0278]]])\n",
      "hidden_states tensor([[ 0.3110,  0.9468],\n",
      "        [ 0.3964, -0.0248],\n",
      "        [-0.8394,  0.3342],\n",
      "        [-0.5307,  0.0278]])\n",
      "routing_weights tensor([[0.4735, 0.3135, 0.2131],\n",
      "        [0.3557, 0.2626, 0.3817],\n",
      "        [0.4059, 0.1468, 0.4473],\n",
      "        [0.3604, 0.1580, 0.4816]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "expert_idx tensor([0])\n",
      "token_idx tensor([0, 1, 2, 3])\n",
      "current_states tensor([[ 0.3110,  0.9468],\n",
      "        [ 0.3964, -0.0248],\n",
      "        [-0.8394,  0.3342],\n",
      "        [-0.5307,  0.0278]])\n",
      "expert mask shape torch.Size([3, 4])\n",
      "rtouting weights shape torch.Size([4, 3])\n",
      "selected_sum tensor([0.7869, 0.7374, 0.8532, 0.8420], grad_fn=<SumBackward1>)\n",
      "selected_weights tensor([0.6017, 0.4823, 0.4758, 0.4280], grad_fn=<DivBackward0>)\n",
      "expert_output tensor([[ 0.3110,  0.9468],\n",
      "        [ 0.3964, -0.0248],\n",
      "        [-0.8394,  0.3342],\n",
      "        [-0.5307,  0.0278]])\n",
      " weighted expert output tensor([[ 0.1871,  0.5697],\n",
      "        [ 0.1912, -0.0120],\n",
      "        [-0.3994,  0.1590],\n",
      "        [-0.2272,  0.0119]], grad_fn=<MulBackward0>)\n",
      "final_hidden_state tensor([[ 0.1871,  0.5697],\n",
      "        [ 0.1912, -0.0120],\n",
      "        [-0.3994,  0.1590],\n",
      "        [-0.2272,  0.0119]], grad_fn=<IndexAddBackward0>)\n",
      "expert_idx tensor([1])\n",
      "token_idx tensor([0])\n",
      "current_states tensor([[0.3110, 0.9468]])\n",
      "expert mask shape torch.Size([3, 4])\n",
      "rtouting weights shape torch.Size([4, 3])\n",
      "selected_sum tensor([0.7869, 0.7374, 0.8532, 0.8420], grad_fn=<SumBackward1>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1] doesn't match the broadcast shape [4]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m MoeBlock = mdl.MoEBlock(config)\n\u001b[32m     30\u001b[39m MoeBlock.eval()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mMoeBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_expert_usage_loss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERT_dev\\src\\neobert\\model\\model.py:240\u001b[39m, in \u001b[36mMoEBlock.forward\u001b[39m\u001b[34m(self, hidden_states, output_expert_usage_loss, inference, inference_dropout_threshold, inference_disable_complex_experts, output_activations)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.routing_strategy == \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    239\u001b[39m     top_p = \u001b[38;5;28mself\u001b[39m.config.min_expert_cumprob_per_token\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     final_hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_top_p_routing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouting_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m output = (final_hidden_states,router_logits,)\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_expert_usage_loss:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERT_dev\\src\\neobert\\model\\model.py:370\u001b[39m, in \u001b[36mMoEBlock._top_p_routing\u001b[39m\u001b[34m(self, hidden_states, hidden_states_shape, routing_weights, top_p)\u001b[39m\n\u001b[32m    368\u001b[39m selected_sum = (routing_weights * expert_mask.permute(\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m)).sum(dim=\u001b[32m1\u001b[39m) \n\u001b[32m    369\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mselected_sum\u001b[39m\u001b[33m\"\u001b[39m,selected_sum)\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[43mselected_weights\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_sum\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-9\u001b[39;49m\n\u001b[32m    371\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mselected_weights\u001b[39m\u001b[33m\"\u001b[39m, selected_weights)  \n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# Apply expert\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: output with shape [1] doesn't match the broadcast shape [4]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a minimal config\n",
    "config = mdl.NeoBERTConfig(\n",
    "    vocab_size=100,            # must be > max index in dummy_input\n",
    "    hidden_size=2,             # divisible by num_attention_heads\n",
    "    num_attention_heads=2,     # divides hidden_size evenly\n",
    "    num_hidden_layers=2,       # small number of layers\n",
    "    intermediate_size=16,      # FFN dim\n",
    "    max_length=16,             # match seq_len\n",
    "    pad_token_id=0,\n",
    "    routing_strategy=\"top_p\",\n",
    "    num_experts_per_tok_inference=1,\n",
    "    min_expert_cumprob_per_token = 0.6,\n",
    "\n",
    "                                                                                # define padding token index\n",
    "    rope=False,                # simpler\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "# Instantiate mode\n",
    "\n",
    "# Generate dummy input (ensure all token indices < vocab_si14ze)\n",
    "batch_size = 1\n",
    "seq_len = 4\n",
    "\n",
    "test_input = torch.randn(batch_size,seq_len,config.hidden_size)\n",
    "test_input_shape = test_input.shape\n",
    "print(\"test_input: \", test_input)\n",
    "\n",
    "MoeBlock = mdl.MoEBlock(config)\n",
    "MoeBlock.eval()\n",
    "\n",
    "MoeBlock(hidden_states = test_input, output_expert_usage_loss = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb6e076",
   "metadata": {},
   "source": [
    "test model with losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25fa6878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import neobert.model as mdl\n",
    "importlib.reload(mdl)\n",
    "import neobert.pretraining.losses as lss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edda9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mif cfg.model.type == \"hetero_moe\":\u001b[39m\n                                      ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# Create a minimal config\n",
    "config_basis = {\n",
    "  \"vocab_size\": 100,\n",
    "  \"hidden_size\": 8,\n",
    "  \"num_attention_heads\": 2,\n",
    "  \"num_hidden_layers\": 2,\n",
    "  \"intermediate_size\": 16,\n",
    "  \"max_length\": 16,\n",
    "  \"pad_token_id\": 0,\n",
    "  \"rope\": False,\n",
    "  \"flash_attention\": False\n",
    "}\n",
    "\n",
    "# Generate dummy input (ensure all token indices < vocab_size)\n",
    "batch_size = 2\n",
    "seq_len = 16\n",
    "\n",
    "# Token indices from 1 to vocab_size-1 (reserve 0 for pad_token)\n",
    "dummy_input = torch.randint(1, config.vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Add some padding tokens at the end (optional)\n",
    "dummy_input[:, -2:] = config.pad_token_id  # last 2 tokens are padding\n",
    "\n",
    "# Generate additive pad mask: 0.0 for valid tokens, -inf for padding\n",
    "pad_mask = (dummy_input != config.pad_token_id).float()\n",
    "pad_mask = torch.where(pad_mask == 1, 0.0, float('-inf'))\n",
    "\n",
    "\n",
    "#mop\n",
    "print(\"MOP forward  pass\")\n",
    "config_mop = {\n",
    "    'routing_strategy': \"top_k\",\n",
    "    'num_experts_per_tok_training': 2,\n",
    "    'cost_based_loss_alpha': 1e-5,\n",
    "    'cost_based_loss_epsilon': 1e-2,\n",
    "    'disable_task_performance_scaling': False\n",
    "}\n",
    "\n",
    "config = mdl.NeoBERTConfig(**config_basis,**config_mop)     # simple\n",
    "model = mdl.NeoBERTLMHead(config)\n",
    "\n",
    "# Run forward pass\n",
    "model.training()\n",
    "model_output = model(dummy_input, pad_mask=pad_mask, output_expert_usage_loss=True)\n",
    "\n",
    "#hetero moe\n",
    "print(\"hetero moe forward pass\")\n",
    "config_mop = {\n",
    "    'routing_strategy': \"top_k\",\n",
    "    'num_experts_per_tok_training': 2,\n",
    "    'cost_based_loss_alpha': 1e-5,\n",
    "    'cost_based_loss_epsilon': 1e-2,\n",
    "    'disable_task_performance_scaling': False\n",
    "}\n",
    "\n",
    "\n",
    "load_balancing_loss = lls.load_balancing_loss(gate_logits = model_output['router_logits'], num_experts = 9)\n",
    "penalty_loss = lls.hmoe_penalty_loss_fn(gate_logits = model_output['router_logits'], num_experts = 9)\n",
    "entropy_loss = lls.hmoe_entropy_loss_fn()\n",
    "\n",
    "\n",
    "# if cfg.model.type == \"homo_moe\":\n",
    "#                         model_output = model(batch[\"input_ids\"], batch.get(\"attention_mask\", None), output_expert_usage_loss=False, output_router_logits=True)\n",
    "#                         train_loss = homo_moe_loss_fn(model_output['logits'], model_output['router_logits'], cfg, batch)\n",
    "#                     if cfg.model.type == \"hetero_moe\":\n",
    "#                         model_output = model(batch[\"input_ids\"], batch.get(\"attention_mask\", None), output_expert_usage_loss=False, output_router_logits=True)\n",
    "#                         train_loss = hetero_moe_loss_fn(model_output['logits'], model_output['router_logits'], cfg, batch)\n",
    "#                     if cfg.model.type == \"mop\":\n",
    "#                         model_output = model(batch[\"input_ids\"], batch.get(\"attention_mask\", None), output_expert_usage_loss=True, output_router_logits=False)\n",
    "#                         train_loss = mop_loss_fn(model_output['logits'], model_output['expert_usage_loss'], cfg, batch) \n",
    "\n",
    "#                     logits = model_output['logits']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5385a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gate_prop:0.04%\n",
      "attention_prop:15.16%\n",
      "embedding_prop:27.02%\n",
      "expert_prop:57.79%\n"
     ]
    }
   ],
   "source": [
    "import neobert.model as mdl  # or your actual import if different\n",
    "\n",
    "# Create a minimal config\n",
    "config = mdl.NeoBERTConfig(        # must be > max index in dummy_input\n",
    "    hidden_size=768,             # divisible by num_attention_heads\n",
    "    num_attention_heads=12,     # divides hidden_size evenly\n",
    "    num_hidden_layers=12,       # small number of layers\n",
    "    intermediate_size=3072,      # FFN dim\n",
    "    max_length=128,             # match seq_len\n",
    "    pad_token_id=0,            # define padding token index\n",
    "    rope=False, \n",
    "    vocab_size = 32768,\n",
    "    expert_sizes = '0,504, 616, 728, 840, 952, 1064, 1176',              # simpler\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "# Instantiate model\n",
    "model = mdl.NeoBERTLMHead(config)\n",
    "# sd = model.state_dict()\n",
    "# for k,v in sd.items(): \n",
    "#     print(k,v.shape)\n",
    "    \n",
    " # plot gradients of the gates and of the experts\n",
    "\n",
    "num_gate_params = 0.0\n",
    "num_expert_params = 0.0\n",
    "num_embedding_params = 0.0\n",
    "num_attention_params = 0.0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "        if \"gate\" in name:\n",
    "            num_gate_params += param.numel()\n",
    "        if \"experts\" in name:\n",
    "            num_expert_params += param.numel()\n",
    "        if \"model.encoder\" in name or \"decoder\" in name or \"model.positional_embedding\" in name:\n",
    "            num_embedding_params += param.numel()\n",
    "        if \"qkv\" in name or \"wo\" in name:\n",
    "            num_attention_params += param.numel()\n",
    "\n",
    "total_num_params = num_gate_params + num_expert_params + num_embedding_params + num_attention_params\n",
    "gate_prop= num_gate_params / total_num_params\n",
    "attention_prop = num_attention_params / total_num_params\n",
    "embedding_prop = num_embedding_params / total_num_params\n",
    "expert_prop = num_expert_params / total_num_params\n",
    "\n",
    "print(f\"gate_prop:{gate_prop*100:.2f}%\")\n",
    "print(f\"attention_prop:{attention_prop*100:.2f}%\")\n",
    "print(f\"embedding_prop:{embedding_prop*100:.2f}%\")\n",
    "print(f\"expert_prop:{expert_prop*100:.2f}%\")\n",
    "\n",
    "# print(\"gate_grad_norm:\", gate_grad_norm ** 0.5 / num_gate_params)\n",
    "# print(\"expert_grad_norm:\", expert_grad_norm ** 0.5 / num_expert_params)\n",
    "# print(\"embedding_grad_norm:\", embedding_grad_norm ** 0.5 / num_embedding_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6f2e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f163cd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384.0\n"
     ]
    }
   ],
   "source": [
    "print(32768/2\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f04fb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0, 252, 308, 364, 420, 476, 532, 588], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([0,504, 616, 728, 840, 952, 1064, 1176])//2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "53022c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_num_params:17013368.0\n",
      "gate_prop:0.07%\n",
      "attention_prop:4.06%\n",
      "embedding_prop:46.51%\n",
      "expert_prop:49.36%\n"
     ]
    }
   ],
   "source": [
    "config = mdl.NeoBERTConfig(        # must be > max index in dummy_input\n",
    "    hidden_size=120,             # divisible by num_attention_heads\n",
    "    num_attention_heads=12,     # divides hidden_size evenly\n",
    "    num_hidden_layers=12,       # small number of layers\n",
    "    intermediate_size=3072,      # FFN dim\n",
    "    max_length=128,             # match seq_len\n",
    "    pad_token_id=0,            # define padding token index\n",
    "    rope=False, \n",
    "    vocab_size = 32768,\n",
    "    expert_sizes = '0, 252, 308, 364, 420, 476, 532, 588',              # simpler\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "model = mdl.NeoBERTLMHead(config)\n",
    "# sd = model.state_dict()\n",
    "# for k,v in sd.items(): \n",
    "#     print(k,v.shape)\n",
    "    \n",
    " # plot gradients of the gates and of the experts\n",
    "\n",
    "num_gate_params = 0.0\n",
    "num_expert_params = 0.0\n",
    "num_embedding_params = 0.0\n",
    "num_attention_params = 0.0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "        if \"gate\" in name:\n",
    "            num_gate_params += param.numel()\n",
    "        if \"experts\" in name:\n",
    "            num_expert_params += param.numel()\n",
    "        if \"model.encoder\" in name or \"decoder\" in name or \"model.positional_embedding\" in name:\n",
    "            num_embedding_params += param.numel()\n",
    "        if \"qkv\" in name or \"wo\" in name:\n",
    "            num_attention_params += param.numel()\n",
    "\n",
    "total_num_params = num_gate_params + num_expert_params + num_embedding_params + num_attention_params\n",
    "gate_prop= num_gate_params / total_num_params\n",
    "attention_prop = num_attention_params / total_num_params\n",
    "embedding_prop = num_embedding_params / total_num_params\n",
    "expert_prop = num_expert_params / total_num_params\n",
    "print(f\"total_num_params:{total_num_params}\")\n",
    "\n",
    "print(f\"gate_prop:{gate_prop*100:.2f}%\")\n",
    "print(f\"attention_prop:{attention_prop*100:.2f}%\")\n",
    "print(f\"embedding_prop:{embedding_prop*100:.2f}%\")\n",
    "print(f\"expert_prop:{expert_prop*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "812ddc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_expert_loss: 0.0019212405540550573\n",
      "initial_penalty_loss: 0.0018007499999999998\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#expert_loss\n",
    "expert_sizes = np.array([0, 252, 308, 364, 420, 476, 532, 588])\n",
    "somme = sum(expert_sizes**2)\n",
    "num_layers = 12\n",
    "num_experts = len(expert_sizes)\n",
    "alpha = 1e-8\n",
    "vocab_size = 30522\n",
    "\n",
    "\n",
    "#expert_loss\n",
    "mlm_loss = -np.log(1/vocab_size)\n",
    "initial_expert_loss = (num_layers*somme*alpha)/(num_experts*mlm_loss)\n",
    "print(\"initial_expert_loss:\", initial_expert_loss)\n",
    "\n",
    "#penalty_loss\n",
    "expert_sizes = np.array([196, 252, 308, 364, 420, 476, 532, 588])\n",
    "coeff_penalty =1.5e-6 #1e-6\n",
    "top_p = 0.4\n",
    "# 0.125*4 = 0.5\n",
    "# 0.125*3 = 0.375\n",
    "# donc a priori il faut 4 experts activs au dbut. \n",
    "num_experts = len(expert_sizes)\n",
    "mean_expert_size = sum(expert_sizes)/num_experts\n",
    "\n",
    "prop_3 = 3.75/4\n",
    "prop_4 = 1-3.75/4\n",
    "\n",
    "initial_penalty_loss = coeff_penalty *(prop_3*3+prop_4*4)*mean_expert_size\n",
    "print(\"initial_penalty_loss:\", initial_penalty_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5335637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1322608\n"
     ]
    }
   ],
   "source": [
    "print(somme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9452425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "508032"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(252**2)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e3f6b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2765952"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(588**2)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65945cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007684962216220229\n"
     ]
    }
   ],
   "source": [
    "\n",
    "expert_sizes = np.array([0,504, 616, 728, 840, 952, 1064, 1176])\n",
    "somme = sum(expert_sizes**2)\n",
    "num_layers = 12\n",
    "num_experts = len(expert_sizes)\n",
    "alpha = 1e-8\n",
    "vocab_size = 30522\n",
    "mlm_loss = -np.log(1/vocab_size)\n",
    "print((num_layers*somme*alpha)/(num_experts*mlm_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d1b948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661304.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a271d55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
      "embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "embeddings.LayerNorm.weight torch.Size([768])\n",
      "embeddings.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "pooler.dense.weight torch.Size([768, 768])\n",
      "pooler.dense.bias torch.Size([768])\n",
      "\n",
      "\n",
      "bert.embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "bert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "cls.predictions.bias torch.Size([30522])\n",
      "cls.predictions.transform.dense.weight torch.Size([768, 768])\n",
      "cls.predictions.transform.dense.bias torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.weight torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertForMaskedLM\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model_LM = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "for s, k in model.named_parameters():\n",
    "    print(s, k.size())\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "for s, k in model_LM.named_parameters():\n",
    "    print(s, k.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab8a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neobert.model as mdl  # or your actual import if different\n",
    "\n",
    "# Create a minimal config\n",
    "config = mdl.NeoBERTConfig(        # must be > max index in dummy_input\n",
    "    hidden_size=16,             # divisible by num_attention_heads\n",
    "    num_attention_heads=2,     # divides hidden_size evenly\n",
    "    num_hidden_layers=4,       # small number of layers\n",
    "    intermediate_size=3072,      # FFN dim\n",
    "    max_length=128,             # match seq_len\n",
    "    pad_token_id=0,            # define padding token index\n",
    "    rope=False, \n",
    "    vocab_size = 30522,\n",
    "    expert_sizes = '0,504, 616, 728, 840, 952, 1064, 1176',              # simpler\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "# Generate dummy input (ensure all token indices < vocab_size)\n",
    "batch_size = 2\n",
    "seq_len = 128\n",
    "num_experts = 8\n",
    "\n",
    "# Token indices from 1 to vocab_size-1 (reserve 0 for pad_token)\n",
    "dummy_input = torch.randint(1, config.vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Add some padding tokens at the end (optional)\n",
    "dummy_input[:, -2:] = config.pad_token_id  # last 2 tokens are padding\n",
    "\n",
    "# print(\"dummy input:\", dummy_input)\n",
    "print(\"dummy input shape:\", dummy_input.shape)\n",
    "\n",
    "# Generate additive pad mask: 0.0 for valid tokens, -inf for padding\n",
    "pad_mask = (dummy_input != config.pad_token_id).float()\n",
    "pad_mask = torch.where(pad_mask == 1, 0.0, float('-inf'))\n",
    "#pad_mask = None\n",
    "\n",
    "#print(pad_mask.view(256,1))\n",
    "\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "# sd = model.state_dict()\n",
    "# for k,v in sd.items(): \n",
    "#     print(k,v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5a508c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5254,  1.1156],\n",
      "        [ 1.0100,  0.7761]])\n",
      "tensor([[-1.5254,  1.1156]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,2)\n",
    "print(a)\n",
    "b = torch.tensor([True, False])\n",
    "\n",
    "print(a[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5d320606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape output neobert: torch.Size([256, 8])\n",
      "target_expert_mask shpe: torch.Size([4, 256, 8])\n",
      "RouterPredictionHead outputs shape: torch.Size([2, 128, 8])\n",
      "RouterPredictionHead outputs shape: torch.Size([256, 8])\n",
      "predicted_routed_logits shape: torch.Size([256, 8])\n",
      "predicted_routed_logits shape: torch.Size([4, 256, 8])\n",
      "tensor([[ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [False]])\n",
      "pad mask shape torch.Size([256])\n",
      "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "        True, True, True, True])\n",
      "predicted routed logits shape torch.Size([4, 256, 8])\n",
      "target expert mask shape torch.Size([4, 256, 8])\n",
      "target expert mask shape torch.Size([4, 256, 8])\n",
      "RouterPredictionHead outputs shape: torch.Size([2, 128, 8])\n",
      "RouterPredictionHead outputs shape: torch.Size([256, 8])\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'stacked_gate_logits' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[173]\u001b[39m\u001b[32m, line 128\u001b[39m\n\u001b[32m    126\u001b[39m predicted_routed_logits = BERTpredictor(dummy_input)\n\u001b[32m    127\u001b[39m predicted_routed_logits = torch.stack(predicted_routed_logits, dim=\u001b[32m0\u001b[39m) \u001b[38;5;66;03m#n_layers, batch_size*seq_length, n_experts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m predicted_expert_mask = \u001b[43mget_expert_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_routed_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pad_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    131\u001b[39m     pad_mask = pad_mask.view(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[173]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mget_expert_mask\u001b[39m\u001b[34m(gate_logits)\u001b[39m\n\u001b[32m     47\u001b[39m     stacked_gate_logits = torch.stack([layer_gate.to(compute_device) \u001b[38;5;28;01mfor\u001b[39;00m layer_gate \u001b[38;5;129;01min\u001b[39;00m gate_logits], dim=\u001b[32m0\u001b[39m)\u001b[38;5;66;03m#n_layers, batch_size*seq_length, n_experts\u001b[39;00m\n\u001b[32m     48\u001b[39m     _,_, num_experts = stacked_gate_logits.shape\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m routing_weights = torch.nn.functional.softmax(\u001b[43mstacked_gate_logits\u001b[49m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     51\u001b[39m top_k = \u001b[32m2\u001b[39m\n\u001b[32m     52\u001b[39m _, selected_experts = torch.topk(routing_weights, top_k, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'stacked_gate_logits' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RouterPredictionHead(nn.Module):\n",
    "        def __init__(self, num_experts, hidden_size, num_hidden_layers):\n",
    "            super().__init__()\n",
    "            self.mlp_block = nn.ModuleList(nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
    "                            nn.Linear(hidden_size, num_experts)) for _ in range(num_hidden_layers))\n",
    "\n",
    "        def forward(self, x): #batch_dim, seq_length, hidden_size\n",
    "            outputs = tuple(mlp(x) for mlp in self.mlp_block)## tuple of n_layers tensors of size batch_dim, seq_length, n_experts\n",
    "            print(\"RouterPredictionHead outputs shape:\", outputs[0].shape)\n",
    "            outputs = tuple(layer_output.view(-1, layer_output.size(-1)) for layer_output in outputs)\n",
    "            print(\"RouterPredictionHead outputs shape:\", outputs[0].shape)\n",
    "            return  outputs  # tuple of n_layers tensors of size batch_dim*seq_length, n_experts\n",
    "\n",
    "class BERTPredictor(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        #pretrained bert params\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        #neobert layer params\n",
    "        num_hidden_layers = cfg.num_hidden_layers\n",
    "        num_experts = len(cfg.expert_sizes.split(\",\"))\n",
    "\n",
    "        self.router_head = RouterPredictionHead(num_experts, hidden_size, num_hidden_layers)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        logits = self.router_head(last_hidden_state)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "def get_expert_mask(gate_logits):\n",
    "    if isinstance(gate_logits, tuple):\n",
    "        compute_device = gate_logits[0].device\n",
    "        stacked_gate_logits = torch.stack([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)#n_layers, batch_size*seq_length, n_experts\n",
    "        _,_, num_experts = stacked_gate_logits.shape\n",
    "    routing_weights = torch.nn.functional.softmax(stacked_gate_logits, dim=-1)\n",
    "\n",
    "    top_k = 2\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "    target_expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts).sum(2)#n_layers, batch_size*seq_length, n_experts\n",
    "\n",
    "    return target_expert_mask #n_layers, batch_size*seq_length, n_experts\n",
    "\n",
    "\n",
    "    #load  pretrained config\n",
    "\n",
    "# get pretrained BERT predictor\n",
    "BERTpredictor = BERTPredictor(config)\n",
    "\n",
    "# get pretrained neobert model\n",
    "neobert_model = mdl.NeoBERTLMHead(config)\n",
    "#freeze params\n",
    "neobert_model.eval()\n",
    "for params in neobert_model.parameters():\n",
    "    params.requires_grad = False\n",
    "#get dataset\n",
    "# define loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "#define optimizer\n",
    "\n",
    "#finetuning on layer prediction\n",
    "\n",
    "\n",
    "\n",
    "#compute the target routing paths\n",
    "neobert_model_output = neobert_model(dummy_input, output_expert_usage_loss=False, output_router_logits=True)\n",
    "target_gate_logits = neobert_model_output['router_logits'] #tuple  of n_layers of batch_size*seq_length,  n_experts\n",
    "print(\"shape output neobert:\", target_gate_logits[0].shape)\n",
    "target_expert_mask = get_expert_mask(target_gate_logits) # n_layers*batch_size*n_seq, n_experts\n",
    "print(\"target_expert_mask shpe:\", target_expert_mask.shape)\n",
    "\n",
    "predicted_routed_logits = BERTpredictor(dummy_input)\n",
    "print(\"predicted_routed_logits shape:\", predicted_routed_logits[0].shape)\n",
    "predicted_routed_logits = torch.stack(predicted_routed_logits, dim=0) #n_layers, batch_size*seq_length, n_experts\n",
    "print(\"predicted_routed_logits shape:\", predicted_routed_logits.shape)\n",
    "\n",
    "\n",
    "#ignore padded tokens\n",
    "if pad_mask is not None:\n",
    "    pad_mask = pad_mask.view(-1, 1)\n",
    "    print(pad_mask)\n",
    "    pad_mask = (pad_mask != float(\"-inf\")).squeeze(-1)\n",
    "    print(\"pad mask shape\", pad_mask.shape)\n",
    "    print(pad_mask)\n",
    "    print(\"predicted routed logits shape\", predicted_routed_logits.shape)\n",
    "    predicted_routed_logits = predicted_routed_logits[:,pad_mask,:]\n",
    "    print(\"target expert mask shape\", target_expert_mask.shape)\n",
    "    target_expert_mask = target_expert_mask[:,pad_mask,:]\n",
    "    print(\"target expert mask shape\", target_expert_mask.shape)\n",
    "\n",
    "#reshape \n",
    "predicted_routed_logits = predicted_routed_logits.view(-1, predicted_routed_logits.size(-1)) #n_layers*batch_size*seq_length, n_experts\n",
    "target_expert_mask = target_expert_mask.view(-1, target_expert_mask.size(-1)) #n_layers*batch_size*seq_length, n_experts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss = loss_fn(predicted_routed_logits, target_expert_mask*1.0)\n",
    "loss.backward()\n",
    "\n",
    "# Update model parameters\n",
    "\n",
    "    #testing\n",
    "BERTpredictor.eval()\n",
    "with torch.no_grad():\n",
    "    total_acc =[]\n",
    "    #target\n",
    "    model_output = neobert_model(dummy_input, output_expert_usage_loss=False, output_router_logits=True)\n",
    "    target_gate_logits = model_output['router_logits'] #tuple  of n_layers of batch_size*seq_length,  n_experts\n",
    "    target_expert_mask = get_expert_mask(target_gate_logits) # n_layers*batch_size*n_seq, n_experts\n",
    "\n",
    "    #prediction\n",
    "    predicted_routed_logits = BERTpredictor(dummy_input)\n",
    "    predicted_routed_logits = torch.stack(predicted_routed_logits, dim=0) #n_layers, batch_size*seq_length, n_experts\n",
    "    predicted_expert_mask = get_expert_mask(predicted_routed_logits)\n",
    "\n",
    "    if pad_mask is not None:\n",
    "        pad_mask = pad_mask.view(-1, 1)\n",
    "        print(pad_mask)\n",
    "        pad_mask = (pad_mask != float(\"-inf\")).squeeze(-1)\n",
    "        print(\"pad mask shape\", pad_mask.shape)\n",
    "        print(pad_mask)\n",
    "        print(\"predicted routed logits shape\", predicted_routed_logits.shape)\n",
    "        predicted_routed_logits = predicted_routed_logits[:,pad_mask,:]\n",
    "        print(\"target expert mask shape\", target_expert_mask.shape)\n",
    "        target_expert_mask = target_expert_mask[:,pad_mask,:]\n",
    "        print(\"target expert mask shape\", target_expert_mask.shape)\n",
    "\n",
    "    #compute accuracy between target mask and predicted mask\n",
    "    layer_accuracy =torch.sum(target_expert_mask*predicted_expert_mask, dim = 1)/torch.sum(target_expert_mask, dim = 1)\n",
    "    mean_accuracy = torch.mean(layer_accuracy)\n",
    "    total_acc.append(mean_accuracy)\n",
    "    mean_accuracy = torch.stack(total_acc).mean()\n",
    "    print(mean_accuracy.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a480ac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer:\n",
      "  vocab_size: 30522\n",
      "model:\n",
      "  loss:\n",
      "    cost_based_loss_alpha: 0.1\n",
      "    cost_based_loss_epsilon: 1.0e-06\n",
      "    disable_task_performance_scaling: false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Create a DictConfig with all keys used in mop_loss_fn\n",
    "cfg = DictConfig({\n",
    "    \"tokenizer\": {\n",
    "        \"vocab_size\": 32768  # Example vocab size for BERT-like tokenizer\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"loss\": {\n",
    "            \"cost_based_loss_alpha\": 1e-5,\n",
    "            \"cost_based_loss_epsilon\": 1e-2,\n",
    "            \"disable_task_performance_scaling\": False\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "# Pretty-print for verification\n",
    "print(OmegaConf.to_yaml(cfg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e3f38a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(3)\n",
    "print(torch.tensor([2,3,a]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd50f3",
   "metadata": {},
   "source": [
    "Test load balancing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "273d3414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional,Dict,Any,Tuple\n",
    "\n",
    "def load_balancing_loss_fn(\n",
    "    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n",
    "    num_experts: Optional[int] = None,\n",
    "    top_k=2,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    ") -> Union[torch.Tensor, int]:\n",
    "    r\"\"\"\n",
    "    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n",
    "\n",
    "    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n",
    "    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n",
    "    experts is too unbalanced.\n",
    "\n",
    "    Args:\n",
    "        gate_logits:\n",
    "            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n",
    "            shape [batch_size X sequence_length, num_experts].\n",
    "        num_experts:\n",
    "            Number of experts\n",
    "        top_k:\n",
    "            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n",
    "            parameter.\n",
    "        attention_mask (`torch.Tensor`, *optional*):\n",
    "            The attention_mask used in forward function\n",
    "            shape [batch_size X sequence_length] if not None.\n",
    "\n",
    "    Returns:\n",
    "        The auxiliary loss.\n",
    "    \"\"\"\n",
    "    if gate_logits is None or not isinstance(gate_logits, tuple):\n",
    "        return 0\n",
    "\n",
    "    if isinstance(gate_logits, tuple):\n",
    "        compute_device = gate_logits[0].device\n",
    "        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n",
    "\n",
    "    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n",
    "\n",
    "    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "\n",
    "    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n",
    "\n",
    "    if attention_mask is None:\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "        print(\"tokens per expert:\", tokens_per_expert)\n",
    "\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "        print(\"router prob per expert:\", router_prob_per_expert)\n",
    "    else:\n",
    "        attention_mask= torch.where(attention_mask == float(0.0), 1, float(0.0)) #original mask is additive-->1,0\n",
    "        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "        print(\"tokens per expert:\", tokens_per_expert)\n",
    "\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "        print(\"router prob per expert:\", router_prob_per_expert)\n",
    "\n",
    "        prod= tokens_per_expert * router_prob_per_expert.unsqueeze(0)\n",
    "        print(prod)\n",
    "        print(torch.sum(prod))\n",
    "        \n",
    "        batch_size, sequence_length = attention_mask.shape\n",
    "        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n",
    "\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n",
    "        expert_attention_mask = (\n",
    "            attention_mask[None, :, :, None, None]\n",
    "            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n",
    "            .reshape(-1, top_k, num_experts)\n",
    "            .to(compute_device)\n",
    "        )\n",
    "\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n",
    "            expert_attention_mask, dim=0\n",
    "        )\n",
    "\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n",
    "        router_per_expert_attention_mask = (\n",
    "            attention_mask[None, :, :, None]\n",
    "            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n",
    "            .reshape(-1, num_experts)\n",
    "            .to(compute_device)\n",
    "        )\n",
    "\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n",
    "            router_per_expert_attention_mask, dim=0\n",
    "        )\n",
    "    print(\"after padding:\")\n",
    "    print(\"tokens per expert:\", tokens_per_expert)\n",
    "    print(\"router prob per expert:\", router_prob_per_expert)\n",
    "    prod= tokens_per_expert * router_prob_per_expert.unsqueeze(0)\n",
    "    print(prod)\n",
    "    print(torch.sum(prod))\n",
    "\n",
    "    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n",
    "    return overall_loss * num_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a9268736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmoe_penalty_loss_fn(\n",
    "    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n",
    "    cfg: Dict[str, Any],\n",
    "    num_experts: Optional[int] = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    ") -> Union[torch.Tensor, int]:\n",
    "    r\"\"\"\n",
    "    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n",
    "\n",
    "    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n",
    "    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n",
    "    experts is too unbalanced.\n",
    "\n",
    "    Args:\n",
    "        gate_logits:\n",
    "            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n",
    "            shape [batch_size X sequence_length, num_experts].\n",
    "        num_experts:\n",
    "            Number of experts\n",
    "        cfg:\n",
    "            The configuration object\n",
    "        attention_mask (`torch.Tensor`, *optional*):\n",
    "            The attention_mask used in forward function\n",
    "            shape [batch_size X sequence_length] if not None.\n",
    "\n",
    "    Returns:\n",
    "        The auxiliary loss.\n",
    "    \"\"\"\n",
    "    if gate_logits is None or not isinstance(gate_logits, tuple):\n",
    "        return 0\n",
    "\n",
    "    if isinstance(gate_logits, tuple):\n",
    "        compute_device = gate_logits[0].device\n",
    "        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n",
    "\n",
    "    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n",
    "    expert_sizes = torch.tensor([int(size) for size in cfg.expert_sizes.split(\",\")], device=compute_device)\n",
    "\n",
    "    if cfg.routing_strategy == \"top_k\":\n",
    "        top_k = 2 #if we wanted  to use top_k for heterogeneous moe\n",
    "        _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n",
    "\n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)# n_tokens, top_k, num_experts\n",
    "        expert_mask = torch.sum(expert_mask, dim=1) * expert_sizes  # n_tokens, num_experts\n",
    "        mean_num_activated_experts = top_k\n",
    "    elif cfg.routing_strategy == \"top_p\":\n",
    "        top_p = cfg.min_expert_cumprob_per_token\n",
    "        sorted_weights, sorted_indices = torch.sort(routing_weights, dim=-1, descending=False)\n",
    "\n",
    "        cum_probs = sorted_weights.cumsum(dim=-1)\n",
    "        mask = cum_probs > 1 - top_p\n",
    "\n",
    "        unsorted_mask = torch.zeros_like(mask, dtype=torch.bool)\n",
    "        #expert_mask = unsorted_mask.scatter(dim=-1, index=sorted_indices, src=mask)*expert_sizes\n",
    "        expert_mask = unsorted_mask.scatter(dim=-1, index=sorted_indices, src=mask).float()#n_layers*batch_size*seq_len, num_experts\n",
    "        mean_num_activated_experts = torch.mean(torch.sum(expert_mask.float(), dim=1), dim=0)\n",
    "        expert_mask *= expert_sizes  # n_tokens, num_experts\n",
    "\n",
    "    if attention_mask is None:\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "\n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "    else:\n",
    "        # tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "\n",
    "        # # Compute the average probability of routing to these experts\n",
    "        # router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "\n",
    "        attention_mask= torch.where(attention_mask == float(0.0), 1, float(0.0)) #original mask is additive-->1,0\n",
    "\n",
    "        batch_size, sequence_length = attention_mask.shape\n",
    "        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n",
    "        print(num_hidden_layers)\n",
    "\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n",
    "        expert_attention_mask = (\n",
    "            attention_mask[None, :, :,None]\n",
    "            .expand((num_hidden_layers, batch_size, sequence_length,num_experts))\n",
    "            .reshape(-1, num_experts)\n",
    "            .to(compute_device)\n",
    "        )\n",
    "\n",
    "        # Compute the percentage of tokens routed to each experts\n",
    "        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n",
    "            expert_attention_mask, dim=0\n",
    "        )\n",
    "        print(\"tokens per expert:\", tokens_per_expert)\n",
    "        print(\"tokens per expert shape:\", tokens_per_expert.shape)\n",
    "\n",
    "        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n",
    "        router_per_expert_attention_mask = (\n",
    "            attention_mask[None, :, :, None]\n",
    "            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n",
    "            .reshape(-1, num_experts)\n",
    "            .to(compute_device)\n",
    "        )\n",
    "        \n",
    "        # Compute the average probability of routing to these experts\n",
    "        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n",
    "            router_per_expert_attention_mask, dim=0\n",
    "        )\n",
    "\n",
    "    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n",
    "    return overall_loss * num_experts, mean_num_activated_experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "607b3dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy input shape: torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a minimal config\n",
    "config = mdl.NeoBERTConfig(        # must be > max index in dummy_input\n",
    "    hidden_size=16,             # divisible by num_attention_heads\n",
    "    num_attention_heads=2,     # divides hidden_size evenly\n",
    "    num_hidden_layers=4,       # small number of layers\n",
    "    intermediate_size=3072,      # FFN dim\n",
    "    max_length=128,             # match seq_len\n",
    "    pad_token_id=0,\n",
    "    routing_strategy = \"top_p\",            # define padding token index\n",
    "    rope=False, \n",
    "    vocab_size = 30522,\n",
    "    expert_sizes = '0,504, 616, 728, 840, 952, 1064, 1176',              # simpler\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "# Generate dummy input (ensure all token indices < vocab_size)\n",
    "batch_size = 2\n",
    "seq_len = 128\n",
    "num_experts = 8\n",
    "\n",
    "# Token indices from 1 to vocab_size-1 (reserve 0 for pad_token)\n",
    "dummy_input = torch.randint(1, config.vocab_size, (batch_size, seq_len))\n",
    "\n",
    "# Add some padding tokens at the end (optional)\n",
    "dummy_input[:, -2:] = config.pad_token_id  # last 2 tokens are padding\n",
    "\n",
    "# print(\"dummy input:\", dummy_input)\n",
    "print(\"dummy input shape:\", dummy_input.shape)\n",
    "\n",
    "# Generate additive pad mask: 0.0 for valid tokens, -inf for padding\n",
    "pad_mask = (dummy_input != config.pad_token_id).float()\n",
    "pad_mask = torch.where(pad_mask == 1, 0.0, float('-inf'))\n",
    "#pad_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3e06c411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tokens per expert: tensor([  0.0000, 242.5000, 306.1667, 371.9445, 438.3333, 444.8333, 526.7222,\n",
      "        595.0000])\n",
      "tokens per expert shape: torch.Size([8])\n",
      "router prob per expert shape: torch.Size([8])\n",
      "tensor(365.7164, grad_fn=<SumBackward0>)\n",
      "tensor(2925.7314, grad_fn=<MulBackward0>)\n",
      "tensor(3.9766)\n"
     ]
    }
   ],
   "source": [
    "neobert_model = mdl.NeoBERTLMHead(config)\n",
    "neobert_model_output = neobert_model(dummy_input, output_expert_usage_loss=False, output_router_logits=True)\n",
    "gate_logits = neobert_model_output[\"router_logits\"]\n",
    "penalty_loss, mean_num_activated_experts = hmoe_penalty_loss_fn(gate_logits,config, num_experts=num_experts, attention_mask=pad_mask)\n",
    "print(penalty_loss)\n",
    "print(mean_num_activated_experts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bacb4a48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dummy_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m neobert_model = mdl.NeoBERTLMHead(config)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m neobert_model_output = neobert_model(\u001b[43mdummy_input\u001b[49m, output_expert_usage_loss=\u001b[38;5;28;01mFalse\u001b[39;00m, output_router_logits=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m gate_logits = neobert_model_output[\u001b[33m\"\u001b[39m\u001b[33mrouter_logits\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      5\u001b[39m load_balancing_loss = load_balancing_loss_fn(gate_logits, num_experts=num_experts, attention_mask=pad_mask)\n",
      "\u001b[31mNameError\u001b[39m: name 'dummy_input' is not defined"
     ]
    }
   ],
   "source": [
    "neobert_model = mdl.NeoBERTLMHead(config)\n",
    "\n",
    "neobert_model_output = neobert_model(dummy_input, output_expert_usage_loss=False, output_router_logits=True)\n",
    "gate_logits = neobert_model_output[\"router_logits\"]\n",
    "load_balancing_loss = load_balancing_loss_fn(gate_logits, num_experts=num_experts, attention_mask=pad_mask)\n",
    "print(load_balancing_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92287ef",
   "metadata": {},
   "source": [
    "Test the difficulty measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a05f094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = mdl.NeoBERTConfig(        # must be > max index in dummy_input\n",
    "    hidden_size=16,             # divisible by num_attention_heads\n",
    "    num_attention_heads=2,     # divides hidden_size evenly\n",
    "    num_hidden_layers=4,       # small number of layers\n",
    "    intermediate_size=3072,      # FFN dim\n",
    "    max_length=128,             # match seq_len\n",
    "    pad_token_id=0,            # define padding token index\n",
    "    rope=False, \n",
    "    vocab_size = 30522,\n",
    "    expert_sizes = '0,504, 616, 728, 840, 952, 1064, 1176',              # simpler\n",
    "    flash_attention=False      # simpler\n",
    ")\n",
    "\n",
    "# Generate dummy input (ensure all token indices < vocab_size)\n",
    "batch_size = 2\n",
    "seq_len = 128\n",
    "num_experts = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c07e8de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0765)\n"
     ]
    }
   ],
   "source": [
    "experts = torch.tensor([0,504, 616, 728, 840, 952, 1064, 1176])\n",
    "somme = torch.sum(experts)\n",
    "a  = (experts/somme)**2\n",
    "print(a.mean()*4)\n",
    "#expected initial value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e41b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b96132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\gruau\\.cache\\huggingface\\hub\\models--textattack--bert-base-uncased-STS-B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "difmeasure_model_name = \"textattack/bert-base-uncased-STS-B\"\n",
    "    #we check that\n",
    "tokenizer = AutoTokenizer.from_pretrained(difmeasure_model_name)\n",
    "difmeasure_model = AutoModelForSequenceClassification.from_pretrained(difmeasure_model_name)\n",
    "difmeasure_model.eval()  # evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24fdd528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)#not sure why\n",
    "from typing import Optional\n",
    "\n",
    "def prepare_and_cache_stsb(tokenizer, cache_dir: Optional[str] = None):\n",
    "    \"\"\"\n",
    "    Download, tokenize, and cache the GLUE STSB dataset with only necessary columns.\n",
    "    Only tokenized columns and label are kept.\n",
    "    \"\"\"\n",
    "    stsb = load_dataset(\"glue\", \"stsb\")  # DatasetDict with splits\n",
    "\n",
    "    def tokenize_fn(batch):\n",
    "        # Tokenize both sentences\n",
    "        tokens = tokenizer(\n",
    "            batch[\"sentence1\"],\n",
    "            batch[\"sentence2\"],\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        # Only add label once\n",
    "        tokens[\"label\"] = batch[\"label\"]\n",
    "        return tokens\n",
    "\n",
    "    # Remove sentence1 and sentence2 from the output\n",
    "    columns_to_remove = [\"sentence1\", \"sentence2\"]\n",
    "    # Also remove any other columns except label and idx if you want\n",
    "    for split in stsb.keys():\n",
    "        # Find all columns except label and idx\n",
    "        all_columns = stsb[split].column_names\n",
    "        remove_cols = [col for col in all_columns if col not in [\"label\", \"idx\"]]\n",
    "        stsb[split] = stsb[split].map(tokenize_fn, batched=True, remove_columns=remove_cols)\n",
    "\n",
    "    # Choose cache dir\n",
    "    if cache_dir is None:\n",
    "        base_dir = Path(\"/data\") if Path(\"/data\").exists() else Path.home()\n",
    "        cache_dir = base_dir / \".pathways_cache\" / \"glue_stsb_tokenized\"\n",
    "    else:\n",
    "        cache_dir = Path(cache_dir)\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Save each split\n",
    "    for split in stsb.keys():\n",
    "        stsb[split].save_to_disk(cache_dir / split)\n",
    "    logger.info(f\"Tokenized STSB saved to {cache_dir}\")\n",
    "\n",
    "def load_cached_stsb(tokenizer, cache_dir: Optional[str] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Load cached tokenized STSB splits from disk.\n",
    "    If not all splits are found, prepare and cache them first.\n",
    "    Returns a dict: {\"train\": Dataset, \"validation\": Dataset, \"test\": Dataset}\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        base_dir = Path(\"/data\") if Path(\"/data\").exists() else Path.home()\n",
    "        cache_dir = base_dir / \".pathways_cache\" / \"glue_stsb_tokenized\"\n",
    "    else:\n",
    "        cache_dir = Path(cache_dir)\n",
    "    splits = [\"train\", \"validation\", \"test\"]\n",
    "    datasets = {}\n",
    "    missing = []\n",
    "    for split in splits:\n",
    "        split_path = cache_dir / split\n",
    "        if split_path.exists():\n",
    "            datasets[split] = load_from_disk(split_path)\n",
    "        else:\n",
    "            missing.append(split)\n",
    "    if missing:\n",
    "        logger.info(f\"Missing splits {missing} in cache. Preparing and caching STSB dataset.\")\n",
    "        prepare_and_cache_stsb(tokenizer, cache_dir=cache_dir)\n",
    "        for split in missing:\n",
    "            split_path = cache_dir / split\n",
    "            datasets[split] = load_from_disk(split_path)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8121a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_dir = Path(\".cache\")\n",
    "tokenizer_text_attack = AutoTokenizer.from_pretrained(difmeasure_model_name)\n",
    "splits = load_cached_stsb(tokenizer_text_attack, cache_dir=cached_dir)\n",
    "\n",
    "stsb_train_dataset = splits[\"train\"]\n",
    "stsb_validation_dataset = splits[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a9f6b18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509, 4510, 4511, 4512, 4513, 4514, 4515, 4516, 4517, 4518, 4519, 4520, 4521, 4522, 4523, 4524, 4525, 4526, 4527, 4528, 4529, 4530, 4531, 4532, 4533, 4534, 4535, 4536, 4537, 4538, 4539, 4540, 4541, 4542, 4543, 4544, 4545, 4546, 4547, 4548, 4549, 4550, 4551, 4552, 4553, 4554, 4555, 4556, 4557, 4558, 4559, 4560, 4561, 4562, 4563, 4564, 4565, 4566, 4567, 4568, 4569, 4570, 4571, 4572, 4573, 4574, 4575, 4576, 4577, 4578, 4579, 4580, 4581, 4582, 4583, 4584, 4585, 4586, 4587, 4588, 4589, 4590, 4591, 4592, 4593, 4594, 4595, 4596, 4597, 4598, 4599, 4600, 4601, 4602, 4603, 4604, 4605, 4606, 4607, 4608, 4609, 4610, 4611, 4612, 4613, 4614, 4615, 4616, 4617, 4618, 4619, 4620, 4621, 4622, 4623, 4624, 4625, 4626, 4627, 4628, 4629, 4630, 4631, 4632, 4633, 4634, 4635, 4636, 4637, 4638, 4639, 4640, 4641, 4642, 4643, 4644, 4645, 4646, 4647, 4648, 4649, 4650, 4651, 4652, 4653, 4654, 4655, 4656, 4657, 4658, 4659, 4660, 4661, 4662, 4663, 4664, 4665, 4666, 4667, 4668, 4669, 4670, 4671, 4672, 4673, 4674, 4675, 4676, 4677, 4678, 4679, 4680, 4681, 4682, 4683, 4684, 4685, 4686, 4687, 4688, 4689, 4690, 4691, 4692, 4693, 4694, 4695, 4696, 4697, 4698, 4699, 4700, 4701, 4702, 4703, 4704, 4705, 4706, 4707, 4708, 4709, 4710, 4711, 4712, 4713, 4714, 4715, 4716, 4717, 4718, 4719, 4720, 4721, 4722, 4723, 4724, 4725, 4726, 4727, 4728, 4729, 4730, 4731, 4732, 4733, 4734, 4735, 4736, 4737, 4738, 4739, 4740, 4741, 4742, 4743, 4744, 4745, 4746, 4747, 4748, 4749, 4750, 4751, 4752, 4753, 4754, 4755, 4756, 4757, 4758, 4759, 4760, 4761, 4762, 4763, 4764, 4765, 4766, 4767, 4768, 4769, 4770, 4771, 4772, 4773, 4774, 4775, 4776, 4777, 4778, 4779, 4780, 4781, 4782, 4783, 4784, 4785, 4786, 4787, 4788, 4789, 4790, 4791, 4792, 4793, 4794, 4795, 4796, 4797, 4798, 4799, 4800, 4801, 4802, 4803, 4804, 4805, 4806, 4807, 4808, 4809, 4810, 4811, 4812, 4813, 4814, 4815, 4816, 4817, 4818, 4819, 4820, 4821, 4822, 4823, 4824, 4825, 4826, 4827, 4828, 4829, 4830, 4831, 4832, 4833, 4834, 4835, 4836, 4837, 4838, 4839, 4840, 4841, 4842, 4843, 4844, 4845, 4846, 4847, 4848, 4849, 4850, 4851, 4852, 4853, 4854, 4855, 4856, 4857, 4858, 4859, 4860, 4861, 4862, 4863, 4864, 4865, 4866, 4867, 4868, 4869, 4870, 4871, 4872, 4873, 4874, 4875, 4876, 4877, 4878, 4879, 4880, 4881, 4882, 4883, 4884, 4885, 4886, 4887, 4888, 4889, 4890, 4891, 4892, 4893, 4894, 4895, 4896, 4897, 4898, 4899, 4900, 4901, 4902, 4903, 4904, 4905, 4906, 4907, 4908, 4909, 4910, 4911, 4912, 4913, 4914, 4915, 4916, 4917, 4918, 4919, 4920, 4921, 4922, 4923, 4924, 4925, 4926, 4927, 4928, 4929, 4930, 4931, 4932, 4933, 4934, 4935, 4936, 4937, 4938, 4939, 4940, 4941, 4942, 4943, 4944, 4945, 4946, 4947, 4948, 4949, 4950, 4951, 4952, 4953, 4954, 4955, 4956, 4957, 4958, 4959, 4960, 4961, 4962, 4963, 4964, 4965, 4966, 4967, 4968, 4969, 4970, 4971, 4972, 4973, 4974, 4975, 4976, 4977, 4978, 4979, 4980, 4981, 4982, 4983, 4984, 4985, 4986, 4987, 4988, 4989, 4990, 4991, 4992, 4993, 4994, 4995, 4996, 4997, 4998, 4999, 5000, 5001, 5002, 5003, 5004, 5005, 5006, 5007, 5008, 5009, 5010, 5011, 5012, 5013, 5014, 5015, 5016, 5017, 5018, 5019, 5020, 5021, 5022, 5023, 5024, 5025, 5026, 5027, 5028, 5029, 5030, 5031, 5032, 5033, 5034, 5035, 5036, 5037, 5038, 5039, 5040, 5041, 5042, 5043, 5044, 5045, 5046, 5047, 5048, 5049, 5050, 5051, 5052, 5053, 5054, 5055, 5056, 5057, 5058, 5059, 5060, 5061, 5062, 5063, 5064, 5065, 5066, 5067, 5068, 5069, 5070, 5071, 5072, 5073, 5074, 5075, 5076, 5077, 5078, 5079, 5080, 5081, 5082, 5083, 5084, 5085, 5086, 5087, 5088, 5089, 5090, 5091, 5092, 5093, 5094, 5095, 5096, 5097, 5098, 5099, 5100, 5101, 5102, 5103, 5104, 5105, 5106, 5107, 5108, 5109, 5110, 5111, 5112, 5113, 5114, 5115, 5116, 5117, 5118, 5119, 5120, 5121, 5122, 5123, 5124, 5125, 5126, 5127, 5128, 5129, 5130, 5131, 5132, 5133, 5134, 5135, 5136, 5137, 5138, 5139, 5140, 5141, 5142, 5143, 5144, 5145, 5146, 5147, 5148, 5149, 5150, 5151, 5152, 5153, 5154, 5155, 5156, 5157, 5158, 5159, 5160, 5161, 5162, 5163, 5164, 5165, 5166, 5167, 5168, 5169, 5170, 5171, 5172, 5173, 5174, 5175, 5176, 5177, 5178, 5179, 5180, 5181, 5182, 5183, 5184, 5185, 5186, 5187, 5188, 5189, 5190, 5191, 5192, 5193, 5194, 5195, 5196, 5197, 5198, 5199, 5200, 5201, 5202, 5203, 5204, 5205, 5206, 5207, 5208, 5209, 5210, 5211, 5212, 5213, 5214, 5215, 5216, 5217, 5218, 5219, 5220, 5221, 5222, 5223, 5224, 5225, 5226, 5227, 5228, 5229, 5230, 5231, 5232, 5233, 5234, 5235, 5236, 5237, 5238, 5239, 5240, 5241, 5242, 5243, 5244, 5245, 5246, 5247, 5248, 5249, 5250, 5251, 5252, 5253, 5254, 5255, 5256, 5257, 5258, 5259, 5260, 5261, 5262, 5263, 5264, 5265, 5266, 5267, 5268, 5269, 5270, 5271, 5272, 5273, 5274, 5275, 5276, 5277, 5278, 5279, 5280, 5281, 5282, 5283, 5284, 5285, 5286, 5287, 5288, 5289, 5290, 5291, 5292, 5293, 5294, 5295, 5296, 5297, 5298, 5299, 5300, 5301, 5302, 5303, 5304, 5305, 5306, 5307, 5308, 5309, 5310, 5311, 5312, 5313, 5314, 5315, 5316, 5317, 5318, 5319, 5320, 5321, 5322, 5323, 5324, 5325, 5326, 5327, 5328, 5329, 5330, 5331, 5332, 5333, 5334, 5335, 5336, 5337, 5338, 5339, 5340, 5341, 5342, 5343, 5344, 5345, 5346, 5347, 5348, 5349, 5350, 5351, 5352, 5353, 5354, 5355, 5356, 5357, 5358, 5359, 5360, 5361, 5362, 5363, 5364, 5365, 5366, 5367, 5368, 5369, 5370, 5371, 5372, 5373, 5374, 5375, 5376, 5377, 5378, 5379, 5380, 5381, 5382, 5383, 5384, 5385, 5386, 5387, 5388, 5389, 5390, 5391, 5392, 5393, 5394, 5395, 5396, 5397, 5398, 5399, 5400, 5401, 5402, 5403, 5404, 5405, 5406, 5407, 5408, 5409, 5410, 5411, 5412, 5413, 5414, 5415, 5416, 5417, 5418, 5419, 5420, 5421, 5422, 5423, 5424, 5425, 5426, 5427, 5428, 5429, 5430, 5431, 5432, 5433, 5434, 5435, 5436, 5437, 5438, 5439, 5440, 5441, 5442, 5443, 5444, 5445, 5446, 5447, 5448, 5449, 5450, 5451, 5452, 5453, 5454, 5455, 5456, 5457, 5458, 5459, 5460, 5461, 5462, 5463, 5464, 5465, 5466, 5467, 5468, 5469, 5470, 5471, 5472, 5473, 5474, 5475, 5476, 5477, 5478, 5479, 5480, 5481, 5482, 5483, 5484, 5485, 5486, 5487, 5488, 5489, 5490, 5491, 5492, 5493, 5494, 5495, 5496, 5497, 5498, 5499, 5500, 5501, 5502, 5503, 5504, 5505, 5506, 5507, 5508, 5509, 5510, 5511, 5512, 5513, 5514, 5515, 5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555, 5556, 5557, 5558, 5559, 5560, 5561, 5562, 5563, 5564, 5565, 5566, 5567, 5568, 5569, 5570, 5571, 5572, 5573, 5574, 5575, 5576, 5577, 5578, 5579, 5580, 5581, 5582, 5583, 5584, 5585, 5586, 5587, 5588, 5589, 5590, 5591, 5592, 5593, 5594, 5595, 5596, 5597, 5598, 5599, 5600, 5601, 5602, 5603, 5604, 5605, 5606, 5607, 5608, 5609, 5610, 5611, 5612, 5613, 5614, 5615, 5616, 5617, 5618, 5619, 5620, 5621, 5622, 5623, 5624, 5625, 5626, 5627, 5628, 5629, 5630, 5631, 5632, 5633, 5634, 5635, 5636, 5637, 5638, 5639, 5640, 5641, 5642, 5643, 5644, 5645, 5646, 5647, 5648, 5649, 5650, 5651, 5652, 5653, 5654, 5655, 5656, 5657, 5658, 5659, 5660, 5661, 5662, 5663, 5664, 5665, 5666, 5667, 5668, 5669, 5670, 5671, 5672, 5673, 5674, 5675, 5676, 5677, 5678, 5679, 5680, 5681, 5682, 5683, 5684, 5685, 5686, 5687, 5688, 5689, 5690, 5691, 5692, 5693, 5694, 5695, 5696, 5697, 5698, 5699, 5700, 5701, 5702, 5703, 5704, 5705, 5706, 5707, 5708, 5709, 5710, 5711, 5712, 5713, 5714, 5715, 5716, 5717, 5718, 5719, 5720, 5721, 5722, 5723, 5724, 5725, 5726, 5727, 5728, 5729, 5730, 5731, 5732, 5733, 5734, 5735, 5736, 5737, 5738, 5739, 5740, 5741, 5742, 5743, 5744, 5745, 5746, 5747, 5748]\n"
     ]
    }
   ],
   "source": [
    "print(stsb_train_dataset['idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bfac90ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "class STSBDataCollator:\n",
    "    \"\"\"\n",
    "    Collates batches for STSB regression task from already-tokenized datasets.\n",
    "    Converts attention_mask to additive mask for NeoBERT.\n",
    "    \"\"\"\n",
    "    def __init__(self, dtype_pad_mask):\n",
    "        self.dtype_pad_mask = dtype_pad_mask\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # batch: list of dicts with tokenized fields\n",
    "        input_ids = torch.stack([torch.tensor(ex[\"input_ids\"]) for ex in batch])\n",
    "        # Standard attention mask: 1 for tokens, 0 for padding\n",
    "        attention_mask = torch.stack([torch.tensor(ex[\"attention_mask\"]) for ex in batch])\n",
    "        # Convert to additive mask: 0 for tokens, -inf for padding\n",
    "        additive_mask = torch.where(attention_mask == 1, 0.0, float('-inf')).to(dtype = self.dtype_pad_mask)\n",
    "        labels = torch.tensor([float(ex[\"label\"]) for ex in batch], dtype=torch.float32)\n",
    "        collated = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": additive_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        # If token_type_ids exist (for BERT), add them\n",
    "        if \"token_type_ids\" in batch[0]:\n",
    "            collated[\"token_type_ids\"] = torch.stack([torch.tensor(ex[\"token_type_ids\"]) for ex in batch])\n",
    "        return collated\n",
    "    \n",
    "stsb_collator = STSBDataCollator(torch.float32)\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    stsb_validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=stsb_collator\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    stsb_train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=stsb_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d4b6d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels', 'token_type_ids'])\n",
      "dict_keys(['input_ids', 'attention_mask', 'labels', 'token_type_ids'])\n"
     ]
    }
   ],
   "source": [
    "i = 0 \n",
    "for batch in train_dataloader:\n",
    "    print(batch.keys())\n",
    "    i += 1\n",
    "    if i >= 2:  # Limit to 5 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11e64d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[1.0501425 1.4354967]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[0.87172544 0.57992953]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[0.68418264 0.7202755 ]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[1.9040196  0.73288196]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[1.2275732 1.2863364]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[1.2624129  0.58607954]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[1.169693 1.309912]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[1.111823  0.3798184]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[0.7505956 1.2515901]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[0.84840643 0.3451038 ]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[0.7931632 0.5216416]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[0.8381428  0.59704244]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}, {'gold': 3.2, 'pred': 0.8381428, 'mse_error': 5.578369}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}, {'gold': 3.2, 'pred': 0.8381428, 'mse_error': 5.578369}, {'gold': 2.167, 'pred': 0.59704244, 'mse_error': 2.464767}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[0.31886882 0.7153861 ]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}, {'gold': 3.2, 'pred': 0.8381428, 'mse_error': 5.578369}, {'gold': 2.167, 'pred': 0.59704244, 'mse_error': 2.464767}, {'gold': 1.0, 'pred': 0.31886882, 'mse_error': 0.4639397}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}, {'gold': 3.2, 'pred': 0.8381428, 'mse_error': 5.578369}, {'gold': 2.167, 'pred': 0.59704244, 'mse_error': 2.464767}, {'gold': 1.0, 'pred': 0.31886882, 'mse_error': 0.4639397}, {'gold': 1.917, 'pred': 0.7153861, 'mse_error': 1.443876}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[0.969659   0.69164324]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}, {'gold': 3.2, 'pred': 0.8381428, 'mse_error': 5.578369}, {'gold': 2.167, 'pred': 0.59704244, 'mse_error': 2.464767}, {'gold': 1.0, 'pred': 0.31886882, 'mse_error': 0.4639397}, {'gold': 1.917, 'pred': 0.7153861, 'mse_error': 1.443876}, {'gold': 4.25, 'pred': 0.969659, 'mse_error': 10.760638}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}, {'gold': 3.2, 'pred': 0.8381428, 'mse_error': 5.578369}, {'gold': 2.167, 'pred': 0.59704244, 'mse_error': 2.464767}, {'gold': 1.0, 'pred': 0.31886882, 'mse_error': 0.4639397}, {'gold': 1.917, 'pred': 0.7153861, 'mse_error': 1.443876}, {'gold': 4.25, 'pred': 0.969659, 'mse_error': 10.760638}, {'gold': 3.0, 'pred': 0.69164324, 'mse_error': 5.3285108}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[0.4899286  0.17332132]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}, {'gold': 3.2, 'pred': 0.8381428, 'mse_error': 5.578369}, {'gold': 2.167, 'pred': 0.59704244, 'mse_error': 2.464767}, {'gold': 1.0, 'pred': 0.31886882, 'mse_error': 0.4639397}, {'gold': 1.917, 'pred': 0.7153861, 'mse_error': 1.443876}, {'gold': 4.25, 'pred': 0.969659, 'mse_error': 10.760638}, {'gold': 3.0, 'pred': 0.69164324, 'mse_error': 5.3285108}, {'gold': 1.0, 'pred': 0.4899286, 'mse_error': 0.26017284}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}, {'gold': 3.2, 'pred': 0.8381428, 'mse_error': 5.578369}, {'gold': 2.167, 'pred': 0.59704244, 'mse_error': 2.464767}, {'gold': 1.0, 'pred': 0.31886882, 'mse_error': 0.4639397}, {'gold': 1.917, 'pred': 0.7153861, 'mse_error': 1.443876}, {'gold': 4.25, 'pred': 0.969659, 'mse_error': 10.760638}, {'gold': 3.0, 'pred': 0.69164324, 'mse_error': 5.3285108}, {'gold': 1.0, 'pred': 0.4899286, 'mse_error': 0.26017284}, {'gold': 0.6, 'pred': 0.17332132, 'mse_error': 0.18205473}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[0.7275821 0.8179134]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}, {'gold': 3.2, 'pred': 0.8381428, 'mse_error': 5.578369}, {'gold': 2.167, 'pred': 0.59704244, 'mse_error': 2.464767}, {'gold': 1.0, 'pred': 0.31886882, 'mse_error': 0.4639397}, {'gold': 1.917, 'pred': 0.7153861, 'mse_error': 1.443876}, {'gold': 4.25, 'pred': 0.969659, 'mse_error': 10.760638}, {'gold': 3.0, 'pred': 0.69164324, 'mse_error': 5.3285108}, {'gold': 1.0, 'pred': 0.4899286, 'mse_error': 0.26017284}, {'gold': 0.6, 'pred': 0.17332132, 'mse_error': 0.18205473}, {'gold': 2.6, 'pred': 0.7275821, 'mse_error': 3.5059485}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}, {'gold': 3.2, 'pred': 0.8381428, 'mse_error': 5.578369}, {'gold': 2.167, 'pred': 0.59704244, 'mse_error': 2.464767}, {'gold': 1.0, 'pred': 0.31886882, 'mse_error': 0.4639397}, {'gold': 1.917, 'pred': 0.7153861, 'mse_error': 1.443876}, {'gold': 4.25, 'pred': 0.969659, 'mse_error': 10.760638}, {'gold': 3.0, 'pred': 0.69164324, 'mse_error': 5.3285108}, {'gold': 1.0, 'pred': 0.4899286, 'mse_error': 0.26017284}, {'gold': 0.6, 'pred': 0.17332132, 'mse_error': 0.18205473}, {'gold': 2.6, 'pred': 0.7275821, 'mse_error': 3.5059485}, {'gold': 5.0, 'pred': 0.8179134, 'mse_error': 17.489847}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "[1.7313212 1.097879 ]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}, {'gold': 3.2, 'pred': 0.8381428, 'mse_error': 5.578369}, {'gold': 2.167, 'pred': 0.59704244, 'mse_error': 2.464767}, {'gold': 1.0, 'pred': 0.31886882, 'mse_error': 0.4639397}, {'gold': 1.917, 'pred': 0.7153861, 'mse_error': 1.443876}, {'gold': 4.25, 'pred': 0.969659, 'mse_error': 10.760638}, {'gold': 3.0, 'pred': 0.69164324, 'mse_error': 5.3285108}, {'gold': 1.0, 'pred': 0.4899286, 'mse_error': 0.26017284}, {'gold': 0.6, 'pred': 0.17332132, 'mse_error': 0.18205473}, {'gold': 2.6, 'pred': 0.7275821, 'mse_error': 3.5059485}, {'gold': 5.0, 'pred': 0.8179134, 'mse_error': 17.489847}, {'gold': 4.6, 'pred': 1.7313212, 'mse_error': 8.229317}]\n",
      "[{'gold': 5.0, 'pred': 1.0501425, 'mse_error': 15.601374}, {'gold': 4.75, 'pred': 1.4354967, 'mse_error': 10.985931}, {'gold': 5.0, 'pred': 0.87172544, 'mse_error': 17.04265}, {'gold': 2.4, 'pred': 0.57992953, 'mse_error': 3.3126566}, {'gold': 2.75, 'pred': 0.68418264, 'mse_error': 4.2676015}, {'gold': 2.615, 'pred': 0.7202755, 'mse_error': 3.5899808}, {'gold': 5.0, 'pred': 1.9040196, 'mse_error': 9.585094}, {'gold': 2.333, 'pred': 0.73288196, 'mse_error': 2.5603774}, {'gold': 3.75, 'pred': 1.2275732, 'mse_error': 6.362637}, {'gold': 5.0, 'pred': 1.2863364, 'mse_error': 13.791297}, {'gold': 3.2, 'pred': 1.2624129, 'mse_error': 3.7542439}, {'gold': 1.583, 'pred': 0.58607954, 'mse_error': 0.9938503}, {'gold': 5.0, 'pred': 1.169693, 'mse_error': 14.671251}, {'gold': 5.0, 'pred': 1.309912, 'mse_error': 13.61675}, {'gold': 4.909, 'pred': 1.111823, 'mse_error': 14.418552}, {'gold': 0.8, 'pred': 0.3798184, 'mse_error': 0.17655258}, {'gold': 2.4, 'pred': 0.7505956, 'mse_error': 2.7205353}, {'gold': 5.0, 'pred': 1.2515901, 'mse_error': 14.050575}, {'gold': 4.0, 'pred': 0.84840643, 'mse_error': 9.932543}, {'gold': 0.636, 'pred': 0.3451038, 'mse_error': 0.08462059}, {'gold': 3.0, 'pred': 0.7931632, 'mse_error': 4.870128}, {'gold': 1.714, 'pred': 0.5216416, 'mse_error': 1.4217185}, {'gold': 3.2, 'pred': 0.8381428, 'mse_error': 5.578369}, {'gold': 2.167, 'pred': 0.59704244, 'mse_error': 2.464767}, {'gold': 1.0, 'pred': 0.31886882, 'mse_error': 0.4639397}, {'gold': 1.917, 'pred': 0.7153861, 'mse_error': 1.443876}, {'gold': 4.25, 'pred': 0.969659, 'mse_error': 10.760638}, {'gold': 3.0, 'pred': 0.69164324, 'mse_error': 5.3285108}, {'gold': 1.0, 'pred': 0.4899286, 'mse_error': 0.26017284}, {'gold': 0.6, 'pred': 0.17332132, 'mse_error': 0.18205473}, {'gold': 2.6, 'pred': 0.7275821, 'mse_error': 3.5059485}, {'gold': 5.0, 'pred': 0.8179134, 'mse_error': 17.489847}, {'gold': 4.6, 'pred': 1.7313212, 'mse_error': 8.229317}, {'gold': 5.0, 'pred': 1.097879, 'mse_error': 15.226549}]\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(attention_mask)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     outputs = \u001b[43mdifmeasure_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#attention_mask = torch.where(attention_mask==0,1, float(0.0))\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     pred_scores = outputs.logits.squeeze(-\u001b[32m1\u001b[39m).cpu().numpy()  \u001b[38;5;66;03m# shape: (batch,)\u001b[39;00m\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(pred_scores)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1668\u001b[39m, in \u001b[36mBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1660\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1661\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1662\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1663\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1664\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1665\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1666\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1668\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1676\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1680\u001b[39m pooled_output = outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1682\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.dropout(pooled_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1140\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1155\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    245\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:640\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m    639\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[39m, in \u001b[36mBertOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    553\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    554\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results = []\n",
    "for batch in val_dataloader:\n",
    "    # Get batch input_ids, attention_mask, labels, idx if available\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    \n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    labels = batch[\"labels\"]\n",
    "    idxs = batch.get(\"idx\", None)\n",
    "    # If you need sentence1/sentence2, you must keep them in the dataset and collator\n",
    "\n",
    "    attention_mask=(attention_mask == 0).long() if attention_mask.dtype != torch.long else attention_mask\n",
    "    print(attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = difmeasure_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask = attention_mask\n",
    "            #attention_mask = torch.where(attention_mask==0,1, float(0.0))\n",
    "            \n",
    "        )\n",
    "        pred_scores = outputs.logits.squeeze(-1).cpu().numpy()  # shape: (batch,)\n",
    "        print(pred_scores)\n",
    "     \n",
    "\n",
    "    gold_scores = labels.cpu().numpy()\n",
    "    mse_errors = (pred_scores - gold_scores) ** 2\n",
    "\n",
    "    # For each sample in batch, collect results\n",
    "    for i in range(len(gold_scores)):\n",
    "        result = {\n",
    "            \"gold\": gold_scores[i],\n",
    "            \"pred\": pred_scores[i],\n",
    "            \"mse_error\": mse_errors[i],\n",
    "        }\n",
    "        if idxs is not None:\n",
    "            result[\"idx\"] = idxs[i].item() if hasattr(idxs[i], \"item\") else idxs[i]\n",
    "        # Optionally add input_ids, sentence1, sentence2 if available\n",
    "        results.append(result)\n",
    "        print(results)\n",
    "    # print(results[-1])  # Print last result for verification\n",
    "\n",
    "df = pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "de93282c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion exceeding 128 tokens: 0/5749 = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Check proportion of validation samples exceeding 128 tokens\n",
    "long_count = 0\n",
    "total_count = 0\n",
    "for example in stsb_train_dataset:\n",
    "    if 0 not in example[\"input_ids\"]:\n",
    "        long_count += 1\n",
    "    total_count += 1\n",
    "print(f\"Proportion exceeding 128 tokens: {long_count}/{total_count} = {long_count/total_count:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b63bdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "neobert_model = mdl.NeoBERTLMHead(config)\n",
    "neobert_backbone = neobert_model.model #ignore the LM head\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "class NeoBERTBackboneWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps NeoBERT backbone to accept HuggingFace-style arguments for LoRA compatibility.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, output_router_logits=True, output_expert_usage_loss=False, **kwargs):\n",
    "        # Map input_ids -> src, attention_mask -> pad_mask\n",
    "        src = input_ids\n",
    "        pad_mask = attention_mask\n",
    "        # Pass through to NeoBERT\n",
    "        return self.backbone(src=src, pad_mask=pad_mask, output_router_logits=output_router_logits, output_expert_usage_loss=output_expert_usage_loss)\n",
    "\n",
    "\n",
    "class NeoBERTForSTSB(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        # Wrap backbone for LoRA compatibility\n",
    "        self.neobert_backbone = NeoBERTBackboneWrapper(neobert_backbone)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.regression_head = nn.Linear(hidden_size, 1)  # single float output\n",
    "        self.config = neobert_backbone.config # not quite same as cfg since its a NeoBERTconfig but we could equivalently have used cfg\n",
    "        self.expert_dims = [int(expert_size) for expert_size in self.config.expert_sizes.split(\",\")]\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        backbone_outputs = self.neobert_backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_expert_usage_loss=True\n",
    "        )\n",
    "\n",
    "        # print(\"input_ids:\", input_ids)\n",
    "        # print(\"attention_mask:\", attention_mask)\n",
    "        # print(\"labels:\", labels)\n",
    "\n",
    "        final_hidden_state = backbone_outputs[\"hidden_representation\"]\n",
    "        router_logits  = backbone_outputs[\"router_logits\"]\n",
    "\n",
    "        pooled_output = final_hidden_state[:, 0, :]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        # # print(\"pooled_output:\", pooled_output)\n",
    "        # print(self.regression_head)\n",
    "        \n",
    "        # Print all regression_head parameters\n",
    "        # print(\"regression_head parameters:\")\n",
    "        # for name, param in self.regression_head.named_parameters():\n",
    "        #     print(f\"{name}: {param.data}\")\n",
    "\n",
    "    # # Print min and max of regression_head parameters\n",
    "    #     params = list(self.regression_head.parameters())\n",
    "    #     if params:\n",
    "    #         weights = params[0]\n",
    "    #         bias = params[1] if len(params) > 1 else None\n",
    "    #         print(\"regression_head weights min:\", weights.min().item(), \"max:\", weights.max().item())\n",
    "    #         if bias is not None:\n",
    "    #             print(\"regression_head bias min:\", bias.min().item(), \"max:\", bias.max().item())\n",
    "\n",
    "        logits = self.regression_head(pooled_output).squeeze(-1)\n",
    "        # print(\"logits:\", logits)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.MSELoss()\n",
    "            loss = loss_fct(logits.float(), labels.float())\n",
    "\n",
    "        expert_usage_loss = self._MoEBlockRoutingCost(router_logits, attention_mask) #(batch_size,)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits, \"expert_usage_loss\": expert_usage_loss}\n",
    "\n",
    "    def _MoEBlockRoutingCost(self,\n",
    "                            router_logits, #tuple of n_layers of tensors of size (batch * sequence_length, n_experts)\n",
    "                            attention_mask #shape (batch,seq_length)\n",
    "                            ):\n",
    "\n",
    "        # Correct stacking of tuple of raw_routing_weights\n",
    "        print(\"router logits shape:\", router_logits[0].shape)\n",
    "        \n",
    "        router_logits = torch.stack(list(router_logits), dim=0) #shape (n_layers, batch * sequence_length, n_experts)\n",
    "        raw_routing_weights = torch.softmax(router_logits, dim=-1)\n",
    "        print(\"raw routing weights shape:\", raw_routing_weights.shape)\n",
    "\n",
    "        if len(self.expert_dims) > 1:\n",
    "            expert_dims_tensor = torch.tensor(self.expert_dims,dtype=torch.float32,\n",
    "                device=raw_routing_weights.device )\n",
    "            normalisation_factor = torch.sum(expert_dims_tensor)\n",
    "            normalised_expert_sizes = expert_dims_tensor / normalisation_factor#prevent overflow\n",
    "            routing_costs = normalised_expert_sizes**self.config.expert_cost_exponent\n",
    "            routing_costs = routing_costs.to(dtype = raw_routing_weights.dtype)#cast back to original dtype\n",
    "            print(\"routing costs shape:\", routing_costs.shape)\n",
    "\n",
    "            n_layers,_,n_experts = raw_routing_weights.shape\n",
    "            batch_size, seq_length = attention_mask.shape\n",
    "            raw_routing_weights = raw_routing_weights.reshape(n_layers, batch_size, seq_length, n_experts)\n",
    "            number_of_tokens_per_seq = seq_length\n",
    "            \n",
    "\n",
    "            #ignore padding_tokens\n",
    "            if attention_mask!= None: #attention_mask has shape (Batch,seq_length) and routing_weights has shape (n_layers, batch * sequence_length, n_experts)\n",
    "                multiplicative_mask = torch.where(attention_mask == 0, 1.0, 0.0).to(dtype = raw_routing_weights.dtype).reshape(1, batch_size, seq_length, 1)\n",
    "                number_of_tokens_per_seq = multiplicative_mask.sum(dim=2).squeeze()\n",
    "                print(\"number of tokens per seq:\", number_of_tokens_per_seq)\n",
    "\n",
    "                raw_routing_weights = raw_routing_weights * multiplicative_mask\n",
    "                print(\"raw routing weights after masking shape:\", raw_routing_weights.shape)\n",
    "                print(\"raw routing weights after masking shape:\", raw_routing_weights)\n",
    "            # Compute usage loss per token\n",
    "            expert_usage_loss = torch.einsum('mijk,k->mij', raw_routing_weights, routing_costs) #(n_layers,batch_size,seq_length)\n",
    "            \n",
    "            print(\"expert usage loss per token shape:\", expert_usage_loss.shape)\n",
    "            #sum across layers\n",
    "            expert_usage_loss = torch.sum(expert_usage_loss, dim=0) #shape (batch_size,seq_length)\n",
    "            print(\"expert usage loss:\", expert_usage_loss)\n",
    "\n",
    "\n",
    "            # Average over all tokens in the sequence\n",
    "            normalised_mean_expert_usage_loss = torch.sum(expert_usage_loss, dim = 1) / number_of_tokens_per_seq #shape (batch_size,)\n",
    "        else:\n",
    "            normalised_mean_expert_usage_loss = None\n",
    "            \n",
    "        print(\"normalised mean expert usage loss shape:\", normalised_mean_expert_usage_loss.shape)\n",
    "        return normalised_mean_expert_usage_loss # the normalisation is not an issue as we care about comparisons but just a reminder it is here\n",
    "\n",
    "    \n",
    "\n",
    "neobert_stsb_model = NeoBERTForSTSB(\n",
    "    hidden_size=config.hidden_size,\n",
    "    dropout_prob=0.1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"qkv\", \"wo\",\"in_proj\", \"out_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=TaskType.FEATURE_EXTRACTION,  # < avoids labels\n",
    "#     )\n",
    "\n",
    "\n",
    "#     # 2. Wrap your predictor model with LoRA\n",
    "# for param in neobert_stsb_model.neobert_backbone.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# neobert_stsb_model.neobert_backbone = get_peft_model(neobert_stsb_model.neobert_backbone, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5adcd1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] WON'T CONVERT torch_dynamo_resume_in_forward_at_32 C:\\Users\\gruau\\AppData\\Local\\Temp\\ipykernel_67272\\1552834711.py line 32 \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] due to: \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 130, in check_compiler_exist_windows\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     subprocess.check_output([compiler, \"/help\"], stderr=subprocess.STDOUT)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 466, in check_output\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     with Popen(*popenargs, **kwargs) as process:\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1446, in _call_user_compiler\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 129, in __call__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\__init__.py\", line 2234, in __call__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1521, in compile_fx\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return aot_autograd(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 72, in __call__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                ^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1350, in fw_compiler_base\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1421, in _fw_compiler_base\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return inner_compile(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 475, in compile_fx_inner\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 85, in debug_wrapper\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 661, in _compile_fx_inner\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1334, in load\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 570, in codegen_and_compile\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1913, in compile_to_fn\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self.compile_to_module().call\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1839, in compile_to_module\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._compile_to_module()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1845, in _compile_to_module\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                                              ^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1784, in codegen\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.scheduler.codegen()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3383, in codegen\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._codegen()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3461, in _codegen\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.get_backend(device).codegen_node(node)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4468, in codegen_node\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     cpp_kernel_proxy = CppKernelProxy(kernel_group)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3466, in __init__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 360, in pick_vec_isa\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     _valid_vec_isa_list: List[VecISA] = valid_vec_isa_list()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 350, in valid_vec_isa_list\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa:\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                                                             ^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 148, in __bool__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self.check_build(VecISA._avx_code)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 101, in check_build\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     extra=_get_isa_dry_compile_fingerprint(self._arch_flags),\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 27, in _get_isa_dry_compile_fingerprint\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiler_info = get_compiler_version_info(get_cpp_compiler())\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                               ^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 144, in get_cpp_compiler\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     check_compiler_exist_windows(compiler)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 135, in check_compiler_exist_windows\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     raise RuntimeError(f\"Compiler: {compiler} is not found.\") from exc\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] RuntimeError: Compiler: cl is not found.\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1064, in __call__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     result = self._inner_convert(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 526, in __call__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _compile(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 924, in _compile\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 666, in compile_inner\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_utils_internal.py\", line 87, in wrapper_function\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return function(*args, **kwargs)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 699, in _compile_inner\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     transformations(instructions, code_options)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 219, in _fn\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return fn(*args, **kwargs)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 634, in transform\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     tracer.run()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2796, in run\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     super().run()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 983, in run\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     while self.step():\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]           ^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 895, in step\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 580, in wrapper\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return handle_graph_break(self, inst, speculation.reason)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 649, in handle_graph_break\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.output.compile_subgraph(self, reason=reason)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1142, in compile_subgraph\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1416, in call_user_compiler\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1465, in _call_user_compiler\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] RuntimeError: Compiler: cl is not found.\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 130, in check_compiler_exist_windows\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     subprocess.check_output([compiler, \"/help\"], stderr=subprocess.STDOUT)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 466, in check_output\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     with Popen(*popenargs, **kwargs) as process:\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1446, in _call_user_compiler\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 129, in __call__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\__init__.py\", line 2234, in __call__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1521, in compile_fx\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return aot_autograd(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 72, in __call__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                ^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1350, in fw_compiler_base\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1421, in _fw_compiler_base\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return inner_compile(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 475, in compile_fx_inner\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 85, in debug_wrapper\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 661, in _compile_fx_inner\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1334, in load\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 570, in codegen_and_compile\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1913, in compile_to_fn\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self.compile_to_module().call\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1839, in compile_to_module\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._compile_to_module()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1845, in _compile_to_module\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                                              ^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1784, in codegen\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.scheduler.codegen()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3383, in codegen\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._codegen()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3461, in _codegen\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.get_backend(device).codegen_node(node)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4468, in codegen_node\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     cpp_kernel_proxy = CppKernelProxy(kernel_group)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3466, in __init__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 360, in pick_vec_isa\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     _valid_vec_isa_list: List[VecISA] = valid_vec_isa_list()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 350, in valid_vec_isa_list\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa:\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                                                             ^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 148, in __bool__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self.check_build(VecISA._avx_code)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 101, in check_build\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     extra=_get_isa_dry_compile_fingerprint(self._arch_flags),\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 27, in _get_isa_dry_compile_fingerprint\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiler_info = get_compiler_version_info(get_cpp_compiler())\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                               ^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 144, in get_cpp_compiler\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     check_compiler_exist_windows(compiler)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 135, in check_compiler_exist_windows\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     raise RuntimeError(f\"Compiler: {compiler} is not found.\") from exc\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] RuntimeError: Compiler: cl is not found.\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1064, in __call__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     result = self._inner_convert(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 526, in __call__\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _compile(\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 924, in _compile\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 666, in compile_inner\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_utils_internal.py\", line 87, in wrapper_function\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return function(*args, **kwargs)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 699, in _compile_inner\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     transformations(instructions, code_options)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 219, in _fn\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return fn(*args, **kwargs)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 634, in transform\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     tracer.run()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2796, in run\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     super().run()\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 983, in run\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     while self.step():\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]           ^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 895, in step\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 580, in wrapper\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return handle_graph_break(self, inst, speculation.reason)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 649, in handle_graph_break\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.output.compile_subgraph(self, reason=reason)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1142, in compile_subgraph\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1416, in call_user_compiler\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1465, in _call_user_compiler\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] RuntimeError: Compiler: cl is not found.\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0823 21:33:21.391000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] WON'T CONVERT torch_dynamo_resume_in__MoEBlockRoutingCost_at_82 C:\\Users\\gruau\\AppData\\Local\\Temp\\ipykernel_67272\\1552834711.py line 82 \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] due to: \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 130, in check_compiler_exist_windows\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     subprocess.check_output([compiler, \"/help\"], stderr=subprocess.STDOUT)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 466, in check_output\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     with Popen(*popenargs, **kwargs) as process:\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1446, in _call_user_compiler\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 129, in __call__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\__init__.py\", line 2234, in __call__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1521, in compile_fx\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return aot_autograd(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 72, in __call__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                ^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1350, in fw_compiler_base\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1421, in _fw_compiler_base\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return inner_compile(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 475, in compile_fx_inner\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 85, in debug_wrapper\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 661, in _compile_fx_inner\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1334, in load\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 570, in codegen_and_compile\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1913, in compile_to_fn\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self.compile_to_module().call\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1839, in compile_to_module\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._compile_to_module()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1845, in _compile_to_module\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                                              ^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1784, in codegen\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.scheduler.codegen()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3383, in codegen\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._codegen()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3461, in _codegen\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.get_backend(device).codegen_node(node)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4468, in codegen_node\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     cpp_kernel_proxy = CppKernelProxy(kernel_group)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3466, in __init__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 360, in pick_vec_isa\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     _valid_vec_isa_list: List[VecISA] = valid_vec_isa_list()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 350, in valid_vec_isa_list\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa:\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                                                             ^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 148, in __bool__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self.check_build(VecISA._avx_code)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 101, in check_build\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     extra=_get_isa_dry_compile_fingerprint(self._arch_flags),\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 27, in _get_isa_dry_compile_fingerprint\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiler_info = get_compiler_version_info(get_cpp_compiler())\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                               ^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 144, in get_cpp_compiler\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     check_compiler_exist_windows(compiler)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 135, in check_compiler_exist_windows\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     raise RuntimeError(f\"Compiler: {compiler} is not found.\") from exc\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] RuntimeError: Compiler: cl is not found.\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1064, in __call__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     result = self._inner_convert(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 526, in __call__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _compile(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 924, in _compile\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 666, in compile_inner\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_utils_internal.py\", line 87, in wrapper_function\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return function(*args, **kwargs)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 699, in _compile_inner\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     transformations(instructions, code_options)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 219, in _fn\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return fn(*args, **kwargs)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 634, in transform\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     tracer.run()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2796, in run\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     super().run()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 983, in run\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     while self.step():\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]           ^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 895, in step\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 580, in wrapper\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return handle_graph_break(self, inst, speculation.reason)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 649, in handle_graph_break\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.output.compile_subgraph(self, reason=reason)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1142, in compile_subgraph\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1416, in call_user_compiler\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1465, in _call_user_compiler\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] RuntimeError: Compiler: cl is not found.\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 130, in check_compiler_exist_windows\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     subprocess.check_output([compiler, \"/help\"], stderr=subprocess.STDOUT)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 466, in check_output\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     with Popen(*popenargs, **kwargs) as process:\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"C:\\Users\\gruau\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] FileNotFoundError: [WinError 2] The system cannot find the file specified\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1446, in _call_user_compiler\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = compiler_fn(gm, self.example_inputs())\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_dynamo.py\", line 129, in __call__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_gm = compiler_fn(gm, example_inputs)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\__init__.py\", line 2234, in __call__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return compile_fx(model_, inputs_, config_patches=self.config)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1521, in compile_fx\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return aot_autograd(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\backends\\common.py\", line 72, in __call__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     cg = aot_module_simplified(gm, example_inputs, **self.kwargs)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1071, in aot_module_simplified\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = dispatch_and_compile()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1056, in dispatch_and_compile\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn, _ = create_aot_dispatcher_function(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 522, in create_aot_dispatcher_function\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _create_aot_dispatcher_function(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 759, in _create_aot_dispatcher_function\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn, fw_metadata = compiler_fn(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                ^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\jit_compile_runtime_wrappers.py\", line 588, in aot_dispatch_autograd\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fw_func = aot_config.fw_compiler(fw_module, adjusted_flat_args)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1350, in fw_compiler_base\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _fw_compiler_base(model, example_inputs, is_inference)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 1421, in _fw_compiler_base\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return inner_compile(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 475, in compile_fx_inner\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return wrap_compiler_debug(_compile_fx_inner, compiler_name=\"inductor\")(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\repro\\after_aot.py\", line 85, in debug_wrapper\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     inner_compiled_fn = compiler_fn(gm, example_inputs)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 661, in _compile_fx_inner\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = FxGraphCache.load(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\codecache.py\", line 1334, in load\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = compile_fx_fn(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 570, in codegen_and_compile\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_graph = fx_codegen_and_compile(gm, example_inputs, **fx_kwargs)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py\", line 878, in fx_codegen_and_compile\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = graph.compile_to_fn()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1913, in compile_to_fn\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self.compile_to_module().call\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1839, in compile_to_module\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._compile_to_module()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1845, in _compile_to_module\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.codegen_with_cpp_wrapper() if self.cpp_wrapper else self.codegen()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                                              ^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\graph.py\", line 1784, in codegen\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.scheduler.codegen()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3383, in codegen\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._codegen()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py\", line 3461, in _codegen\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.get_backend(device).codegen_node(node)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 4468, in codegen_node\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     cpp_kernel_proxy = CppKernelProxy(kernel_group)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py\", line 3466, in __init__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.picked_vec_isa: cpu_vec_isa.VecISA = cpu_vec_isa.pick_vec_isa()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 360, in pick_vec_isa\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     _valid_vec_isa_list: List[VecISA] = valid_vec_isa_list()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                         ^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 350, in valid_vec_isa_list\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     if all(flag in _cpu_supported_x86_isa for flag in str(isa).split()) and isa:\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                                                             ^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 148, in __bool__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self.check_build(VecISA._avx_code)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 101, in check_build\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     extra=_get_isa_dry_compile_fingerprint(self._arch_flags),\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py\", line 27, in _get_isa_dry_compile_fingerprint\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiler_info = get_compiler_version_info(get_cpp_compiler())\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                                               ^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 144, in get_cpp_compiler\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     check_compiler_exist_windows(compiler)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_inductor\\cpp_builder.py\", line 135, in check_compiler_exist_windows\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     raise RuntimeError(f\"Compiler: {compiler} is not found.\") from exc\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] RuntimeError: Compiler: cl is not found.\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] The above exception was the direct cause of the following exception:\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Traceback (most recent call last):\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 1064, in __call__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     result = self._inner_convert(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]              ^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 526, in __call__\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _compile(\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 924, in _compile\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 666, in compile_inner\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return _compile_inner(code, one_graph, hooks, transform)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_utils_internal.py\", line 87, in wrapper_function\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return function(*args, **kwargs)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 699, in _compile_inner\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     out_code = transform_code_object(code, transform)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py\", line 1322, in transform_code_object\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     transformations(instructions, code_options)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 219, in _fn\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return fn(*args, **kwargs)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py\", line 634, in transform\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     tracer.run()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 2796, in run\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     super().run()\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 983, in run\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     while self.step():\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]           ^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 895, in step\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.dispatch_table[inst.opcode](self, inst)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 580, in wrapper\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return handle_graph_break(self, inst, speculation.reason)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py\", line 649, in handle_graph_break\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.output.compile_subgraph(self, reason=reason)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1142, in compile_subgraph\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1369, in compile_and_call_fx_graph\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     compiled_fn = self.call_user_compiler(gm)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1416, in call_user_compiler\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     return self._call_user_compiler(gm)\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]   File \"c:\\Users\\gruau\\OneDrive\\Documents\\CentraleSupelec\\3A\\Stage Oxford\\Implementation\\NeoBERT\\NeoBERTenv\\Lib\\site-packages\\torch\\_dynamo\\output_graph.py\", line 1465, in _call_user_compiler\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125]     raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] RuntimeError: Compiler: cl is not found.\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "W0823 21:33:21.667000 67272 Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1125] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "router logits shape: torch.Size([256, 8])\n",
      "raw routing weights shape: torch.Size([4, 256, 8])\n",
      "routing costs shape: torch.Size([8])\n",
      "number of tokens per seq: tensor([16., 18.])\n",
      "raw routing weights after masking shape: torch.Size([4, 2, 128, 8])\n",
      "raw routing weights after masking shape: tensor([[[[0.1211, 0.1335, 0.1303,  ..., 0.1242, 0.1222, 0.1211],\n",
      "          [0.1295, 0.1284, 0.1280,  ..., 0.1269, 0.1230, 0.1145],\n",
      "          [0.1230, 0.1227, 0.1252,  ..., 0.1191, 0.1300, 0.1293],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1209, 0.1335, 0.1301,  ..., 0.1241, 0.1226, 0.1214],\n",
      "          [0.1294, 0.1283, 0.1278,  ..., 0.1268, 0.1234, 0.1147],\n",
      "          [0.1242, 0.1314, 0.1313,  ..., 0.1201, 0.1247, 0.1162],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1178, 0.1243, 0.1311,  ..., 0.1290, 0.1315, 0.1194],\n",
      "          [0.1238, 0.1221, 0.1142,  ..., 0.1353, 0.1242, 0.1284],\n",
      "          [0.1264, 0.1229, 0.1158,  ..., 0.1316, 0.1231, 0.1314],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1177, 0.1243, 0.1311,  ..., 0.1292, 0.1316, 0.1196],\n",
      "          [0.1238, 0.1220, 0.1141,  ..., 0.1355, 0.1242, 0.1285],\n",
      "          [0.1211, 0.1310, 0.1235,  ..., 0.1225, 0.1240, 0.1295],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1159, 0.1284, 0.1277,  ..., 0.1204, 0.1317, 0.1286],\n",
      "          [0.1161, 0.1197, 0.1273,  ..., 0.1256, 0.1306, 0.1210],\n",
      "          [0.1207, 0.1327, 0.1286,  ..., 0.1317, 0.1267, 0.1165],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1158, 0.1284, 0.1280,  ..., 0.1207, 0.1318, 0.1285],\n",
      "          [0.1160, 0.1196, 0.1276,  ..., 0.1259, 0.1307, 0.1208],\n",
      "          [0.1138, 0.1367, 0.1277,  ..., 0.1250, 0.1268, 0.1231],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1374, 0.1228, 0.1228,  ..., 0.1246, 0.1298, 0.1207],\n",
      "          [0.1326, 0.1194, 0.1270,  ..., 0.1261, 0.1313, 0.1248],\n",
      "          [0.1249, 0.1276, 0.1205,  ..., 0.1262, 0.1304, 0.1348],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1375, 0.1229, 0.1226,  ..., 0.1249, 0.1301, 0.1206],\n",
      "          [0.1326, 0.1194, 0.1268,  ..., 0.1264, 0.1316, 0.1248],\n",
      "          [0.1297, 0.1255, 0.1227,  ..., 0.1192, 0.1243, 0.1339],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "expert usage loss per token shape: torch.Size([4, 2, 128])\n",
      "expert usage loss: tensor([[0.0764, 0.0766, 0.0771, 0.0761, 0.0762, 0.0762, 0.0762, 0.0758, 0.0766,\n",
      "         0.0767, 0.0774, 0.0768, 0.0764, 0.0766, 0.0764, 0.0763, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0764, 0.0766, 0.0762, 0.0762, 0.0766, 0.0760, 0.0763, 0.0762, 0.0766,\n",
      "         0.0765, 0.0762, 0.0770, 0.0767, 0.0770, 0.0762, 0.0767, 0.0771, 0.0762,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<SumBackward1>)\n",
      "normalised mean expert usage loss shape: torch.Size([2])\n",
      "Loss: 16.451597213745117\n",
      "Expert usage loss: tensor([0.0765, 0.0765], grad_fn=<DivBackward0>)\n",
      "router logits shape: torch.Size([256, 8])\n",
      "raw routing weights shape: torch.Size([4, 256, 8])\n",
      "routing costs shape: torch.Size([8])\n",
      "number of tokens per seq: tensor([28., 15.])\n",
      "raw routing weights after masking shape: torch.Size([4, 2, 128, 8])\n",
      "raw routing weights after masking shape: tensor([[[[0.1209, 0.1336, 0.1302,  ..., 0.1244, 0.1225, 0.1211],\n",
      "          [0.1293, 0.1284, 0.1278,  ..., 0.1271, 0.1233, 0.1144],\n",
      "          [0.1241, 0.1316, 0.1313,  ..., 0.1204, 0.1247, 0.1159],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1214, 0.1334, 0.1303,  ..., 0.1241, 0.1220, 0.1211],\n",
      "          [0.1328, 0.1202, 0.1206,  ..., 0.1220, 0.1247, 0.1199],\n",
      "          [0.1274, 0.1230, 0.1251,  ..., 0.1240, 0.1266, 0.1177],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1178, 0.1243, 0.1311,  ..., 0.1291, 0.1314, 0.1196],\n",
      "          [0.1239, 0.1220, 0.1141,  ..., 0.1354, 0.1240, 0.1285],\n",
      "          [0.1212, 0.1310, 0.1235,  ..., 0.1225, 0.1239, 0.1295],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1178, 0.1243, 0.1311,  ..., 0.1287, 0.1315, 0.1194],\n",
      "          [0.1345, 0.1291, 0.1236,  ..., 0.1283, 0.1167, 0.1208],\n",
      "          [0.1286, 0.1294, 0.1218,  ..., 0.1260, 0.1157, 0.1304],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1159, 0.1282, 0.1276,  ..., 0.1207, 0.1317, 0.1286],\n",
      "          [0.1161, 0.1195, 0.1272,  ..., 0.1260, 0.1307, 0.1209],\n",
      "          [0.1139, 0.1366, 0.1274,  ..., 0.1251, 0.1268, 0.1232],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1160, 0.1285, 0.1279,  ..., 0.1199, 0.1315, 0.1287],\n",
      "          [0.1299, 0.1279, 0.1177,  ..., 0.1211, 0.1190, 0.1168],\n",
      "          [0.1173, 0.1327, 0.1217,  ..., 0.1305, 0.1233, 0.1211],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.1374, 0.1229, 0.1228,  ..., 0.1248, 0.1300, 0.1205],\n",
      "          [0.1326, 0.1195, 0.1270,  ..., 0.1263, 0.1314, 0.1247],\n",
      "          [0.1297, 0.1255, 0.1229,  ..., 0.1191, 0.1242, 0.1339],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.1374, 0.1226, 0.1227,  ..., 0.1247, 0.1298, 0.1207],\n",
      "          [0.1245, 0.1269, 0.1257,  ..., 0.1161, 0.1175, 0.1306],\n",
      "          [0.1232, 0.1300, 0.1240,  ..., 0.1162, 0.1186, 0.1372],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "expert usage loss per token shape: torch.Size([4, 2, 128])\n",
      "expert usage loss: tensor([[0.0764, 0.0766, 0.0762, 0.0762, 0.0763, 0.0766, 0.0764, 0.0762, 0.0767,\n",
      "         0.0770, 0.0762, 0.0771, 0.0766, 0.0765, 0.0761, 0.0766, 0.0771, 0.0763,\n",
      "         0.0761, 0.0771, 0.0766, 0.0765, 0.0769, 0.0764, 0.0767, 0.0770, 0.0764,\n",
      "         0.0763, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000],\n",
      "        [0.0764, 0.0755, 0.0763, 0.0761, 0.0765, 0.0763, 0.0762, 0.0758, 0.0764,\n",
      "         0.0767, 0.0766, 0.0771, 0.0765, 0.0767, 0.0762, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<SumBackward1>)\n",
      "normalised mean expert usage loss shape: torch.Size([2])\n",
      "Loss: 8.342567443847656\n",
      "Expert usage loss: tensor([0.0765, 0.0763], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def to_target_batch_size(\n",
    "    batch,\n",
    "    stored_batch ,\n",
    "    target_size: int = 8,\n",
    "    ):\n",
    "    tmp = {}\n",
    "    batch_size = batch[\"input_ids\"].shape[0]\n",
    "\n",
    "    # If the batch is toO large, we store samples\n",
    "    if batch_size > target_size:\n",
    "        for key in batch.keys():\n",
    "            tmp[key] = torch.split(batch[key], [target_size, batch_size - target_size], dim=0)\n",
    "            batch[key] = tmp[key][0]\n",
    "            stored_batch[key] = tmp[key][1] if stored_batch[key] is None else torch.cat([tmp[key][1], stored_batch[key]], dim=0)\n",
    "\n",
    "    # If the batch is to small, we fetch stored samples\n",
    "    elif batch_size < target_size and stored_batch[\"input_ids\"] is not None:\n",
    "        stored_batch_size = stored_batch[\"input_ids\"].shape[0]\n",
    "        missing = target_size - batch_size\n",
    "\n",
    "        # Fetch only necessary samples if storage is larger than required\n",
    "        if missing < stored_batch_size:\n",
    "            for key in batch.keys():\n",
    "                stored_batch[key].to(batch[key].device)\n",
    "                tmp[key] = torch.split(stored_batch[key], [missing, stored_batch_size - missing], dim=0)\n",
    "                batch[key] = torch.cat([batch[key], tmp[key][0]], dim=0)\n",
    "                stored_batch[key] = tmp[key][1]\n",
    "                stored_batch[key].to(\"cpu\", non_blocking=True)\n",
    "\n",
    "        # Concatenate otherwise\n",
    "        else:\n",
    "            for key in batch.keys():\n",
    "                batch[key] = torch.cat([batch[key], stored_batch[key]], dim=0)\n",
    "                stored_batch[key] = None\n",
    "\n",
    "    return batch, stored_batch\n",
    "\n",
    "\n",
    "compiled_neobert_stsb_model = torch.compile(neobert_stsb_model)\n",
    "stored_batch = {\n",
    "    \"input_ids\": None,\n",
    "    \"attention_mask\": None,\n",
    "    \"labels\": None,\n",
    "}\n",
    "\n",
    "count = 1\n",
    "for batch in train_dataloader:\n",
    "\n",
    "    if batch[\"input_ids\"].shape[0] != batch_size:\n",
    "        batch, stored_batch = to_target_batch_size(batch, stored_batch, batch_size)\n",
    "    if batch[\"input_ids\"].shape[0] < batch_size:\n",
    "        stored_batch = batch\n",
    "        continue\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = compiled_neobert_stsb_model(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        labels=batch[\"labels\"]\n",
    "    )\n",
    "    loss = outputs[\"loss\"]  # This is already MSE loss from your model's forward()\n",
    "    print(\"Loss:\", loss.item())\n",
    "    print(\"Expert usage loss:\", outputs[\"expert_usage_loss\"])\n",
    "    count += 1\n",
    "    if count > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a3a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e13a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "seq_len = 128\n",
    "a = torch.randn(batch_size, seq_len)\n",
    "b = torch.randn(batch_size, seq_len)\n",
    "dummy_input = (a, b)\n",
    "_MoEBlockRoutingCost()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17d48de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing rewrite function with STSB tokenizer:\n",
      "\n",
      "Sample 1:\n",
      "Original token IDs: [101, 1037, 2158, 3564, 2006, 1996, 2723, 3248, 1037, 2858, 1012, 102, 1037, 2158, 3564, 2006, 1996, 2723, 1999, 1037]...\n",
      "Valid token IDs: [101, 1037, 2158, 3564, 2006, 1996, 2723, 3248, 1037, 2858, 1012, 102, 1037, 2158, 3564, 2006, 1996, 2723, 1999, 1037]...\n",
      "Rewritten text: a man sitting on the floor plays a guitar. a man sitting on the floor in a room is strumming a guitar.\n",
      "\n",
      "Sample 2:\n",
      "Original token IDs: [101, 1037, 17022, 3248, 2007, 1037, 6081, 11661, 1012, 102, 1996, 3899, 2003, 2652, 2007, 1037, 6081, 11661, 1012, 102]...\n",
      "Valid token IDs: [101, 1037, 17022, 3248, 2007, 1037, 6081, 11661, 1012, 102, 1996, 3899, 2003, 2652, 2007, 1037, 6081, 11661, 1012, 102]...\n",
      "Rewritten text: a puppy plays with a plastic container. the dog is playing with a plastic container.\n"
     ]
    }
   ],
   "source": [
    "def rewrite_inputs_with_tokenizer(batch, tokenizer):\n",
    "    \"\"\"\n",
    "    Takes tokenized inputs and rewrites/detokenizes them back to text using the tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        batch: Dictionary containing 'input_ids' and optionally 'attention_mask'\n",
    "        tokenizer: The tokenizer used for encoding/decoding\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with original inputs and rewritten text\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch.get(\"attention_mask\", None)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(input_ids.shape[0]):\n",
    "        # Get individual sequence\n",
    "        sequence_ids = input_ids[i]\n",
    "        \n",
    "        # If attention mask exists, use it to identify valid tokens\n",
    "        if attention_mask is not None:\n",
    "            # Convert additive mask back to standard mask for processing\n",
    "            if attention_mask.dtype == torch.float32:\n",
    "                # Additive mask: 0.0 for valid tokens, -inf for padding\n",
    "                valid_mask = (attention_mask[i] != float('-inf'))\n",
    "            else:\n",
    "                # Standard mask: 1 for valid tokens, 0 for padding\n",
    "                valid_mask = (attention_mask[i] == 1)\n",
    "            \n",
    "            # Only keep valid tokens\n",
    "            valid_ids = sequence_ids[valid_mask]\n",
    "        else:\n",
    "            valid_ids = sequence_ids\n",
    "        \n",
    "        # Decode the sequence back to text\n",
    "        decoded_text = tokenizer.decode(valid_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'original_ids': sequence_ids.tolist(),\n",
    "            'valid_ids': valid_ids.tolist(),\n",
    "            'rewritten_text': decoded_text\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage with the existing batch\n",
    "print(\"Testing rewrite function with STSB tokenizer:\")\n",
    "example_results = rewrite_inputs_with_tokenizer(batch, tokenizer_text_attack)\n",
    "\n",
    "for i, result in enumerate(example_results):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Original token IDs: {result['original_ids'][:20]}...\")  # Show first 20 tokens\n",
    "    print(f\"Valid token IDs: {result['valid_ids'][:20]}...\")\n",
    "    print(f\"Rewritten text: {result['rewritten_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b0ce4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Dict, Any\n",
    "def get_entropy(gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None], cfg, attention_mask):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the entropy loss for the Heterogeneous Mixture of Experts (HMoE) model.\n",
    "\n",
    "    Args:\n",
    "        gate_logits: Logits from the gate, should be a tensor of shape [batch_size, sequence_length, num_experts].\n",
    "        cfg: Configuration object containing model parameters.\n",
    "        batch: The input batch containing labels and other necessary information.\n",
    "\n",
    "    Returns:\n",
    "        The computed entropy loss.\n",
    "    \"\"\"\n",
    "    if isinstance(gate_logits, tuple):\n",
    "        compute_device = gate_logits[0].device\n",
    "        if attention_mask is None:\n",
    "            concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)#n_layers*batch_size*seq_length,num_expert\n",
    "        else:\n",
    "            concatenated_gate_logits = torch.stack([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)#n_layers,batch_size*seq_length,num_experts\n",
    "            boolean_pad_mask = (attention_mask.reshape(-1) != float(\"-inf\")).squeeze(-1)#[batch_size*seq_length,num_experts]\n",
    "            concatenated_gate_logits = concatenated_gate_logits[:,boolean_pad_mask,:].reshape(-1, concatenated_gate_logits.shape[-1]) #n_layers*batch_size*seq_length,num_expert\n",
    "\n",
    "    else: \n",
    "        print(\"WARNING: gate_logits is not a tuple, this is not supported in the current implementation of entropy loss function. \")\n",
    "\n",
    "    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n",
    "\n",
    "    entropy_loss_epsilon = 1e-5\n",
    "    \n",
    "    # Compute the entropy\n",
    "    entropy_loss = -torch.sum(routing_weights * torch.log(routing_weights + entropy_loss_epsilon), dim=-1)\n",
    "\n",
    "    # Average\n",
    "    entropy_loss = torch.mean(entropy_loss)\n",
    "\n",
    "    return entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a635788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 2\n",
    "n_seq = 4\n",
    "n_batch = 3\n",
    "n_experts = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "408c32fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform probabilities: tensor([0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250])\n",
      "Manual entropy per token: tensor(2.0794)\n",
      "Average manual entropy: tensor(2.0794)\n",
      "Theoretical maximum entropy: 2.0794415416798357\n",
      "Your function result: tensor(2.0794)\n"
     ]
    }
   ],
   "source": [
    "# Test entropy calculation with uniform logits\n",
    "import math\n",
    "\n",
    "# Create uniform logits for 8 experts\n",
    "n_experts = 8\n",
    "n_tokens = 100  # arbitrary number of tokens\n",
    "\n",
    "# Uniform logits (all zeros will give uniform probabilities after softmax)\n",
    "uniform_logits = torch.zeros(n_tokens, n_experts)\n",
    "\n",
    "# Calculate entropy manually\n",
    "uniform_probs = torch.nn.functional.softmax(uniform_logits, dim=-1)\n",
    "print(\"Uniform probabilities:\", uniform_probs[0])  # Should be [0.125, 0.125, ...]\n",
    "\n",
    "# Manual entropy calculation\n",
    "manual_entropy = -torch.sum(uniform_probs * torch.log(uniform_probs + 1e-5), dim=-1)\n",
    "print(\"Manual entropy per token:\", manual_entropy[0])\n",
    "print(\"Average manual entropy:\", torch.mean(manual_entropy))\n",
    "\n",
    "# Theoretical maximum entropy\n",
    "theoretical_max = math.log(n_experts)\n",
    "print(\"Theoretical maximum entropy:\", theoretical_max)\n",
    "\n",
    "# Test with your function\n",
    "gate_logits = (uniform_logits,)  # Tuple format as expected by your function\n",
    "attention_mask = None  # No masking\n",
    "\n",
    "entropy_result = get_entropy(gate_logits, None, attention_mask)\n",
    "print(\"Your function result:\", entropy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ad15815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOGARITHM CLARIFICATION ===\n",
      "Natural log (ln) of 8: 2.079442\n",
      "Log base 2 of 8: 3.000000\n",
      "Log base 10 of 8: 0.903090\n",
      "\n",
      "=== ENTROPY UNITS ===\n",
      "- Natural log (ln): measured in 'nats'\n",
      "- Log base 2: measured in 'bits'\n",
      "- Log base 10: measured in 'dits' or 'bans'\n",
      "\n",
      "For uniform distribution over 8 experts:\n",
      "- Maximum entropy in nats: 2.079442\n",
      "- Maximum entropy in bits: 3.000000\n",
      "\n",
      "PyTorch uses natural log by default:\n",
      "torch.log(8) = 2.079442\n",
      "\n",
      "Your entropy result of ~2.08 is correct in NATS!\n",
      "If you want bits, convert: 3.000000 bits\n"
     ]
    }
   ],
   "source": [
    "# Clarify the logarithm confusion\n",
    "import math\n",
    "\n",
    "print(\"=== LOGARITHM CLARIFICATION ===\")\n",
    "print(f\"Natural log (ln) of 8: {math.log(8):.6f}\")\n",
    "print(f\"Log base 2 of 8: {math.log2(8):.6f}\")\n",
    "print(f\"Log base 10 of 8: {math.log10(8):.6f}\")\n",
    "\n",
    "print(\"\\n=== ENTROPY UNITS ===\")\n",
    "print(\"- Natural log (ln): measured in 'nats'\")\n",
    "print(\"- Log base 2: measured in 'bits'\") \n",
    "print(\"- Log base 10: measured in 'dits' or 'bans'\")\n",
    "\n",
    "print(f\"\\nFor uniform distribution over 8 experts:\")\n",
    "print(f\"- Maximum entropy in nats: {math.log(8):.6f}\")\n",
    "print(f\"- Maximum entropy in bits: {math.log2(8):.6f}\")\n",
    "\n",
    "print(f\"\\nPyTorch uses natural log by default:\")\n",
    "print(f\"torch.log(8) = {torch.log(torch.tensor(8.0)).item():.6f}\")\n",
    "\n",
    "print(f\"\\nYour entropy result of ~2.08 is correct in NATS!\")\n",
    "print(f\"If you want bits, convert: {math.log(8) / math.log(2):.6f} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5200538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1387680.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "experts = [0, 252, 308, 364, 420, 476]\n",
    "mean_expert_usage= np.mean([(expert)**2 for expert in experts])\n",
    "print(mean_expert_usage*12)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
