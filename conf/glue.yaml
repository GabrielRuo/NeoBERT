defaults:
  - dataset: glue
  - tokenizer: google
  - model: 
    - neobert
    - 120M
  - optimizer: adamw
  - trainer: glue
  - scheduler: glue
  - dataloader: base
  - datacollator: base
  - _self_

seed: 0

meta_task: glue
task: mnli
id: neobert_1000000_mnli_8_1e-5_0
mode: train # eval, inference

hydra:
  run:
    dir: logs/hydra

wandb:
  name: neobert-mnli
  project: neo-bert_glue
  entity: sarath-chandar
  tags: ["finetuning"]
  dir: null
  mode: offline
  resume: True

model:
  name: neobert
  from_hub: false
  classifier_init_range: 0.02
  classifier_dropout: 0.1
  pretrained_config_path: ${oc.env:SCRATCH}/logs/neobert/hydra/.hydra/config.yaml
  pretrained_checkpoint_dir: ${oc.env:SCRATCH}/logs/neobert/model_checkpoints
  pretrained_checkpoint: 1000000
  num_checkpoints_to_merge: 1
  transfer_from_task: false
  matching_task:
    mnli: snli
    wnli: mnli
    qnli: mnli

